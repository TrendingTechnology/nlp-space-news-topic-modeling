{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scraping news article text](#scraping-news-articles-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from time import sleep, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.scraping_helpers\n",
    "from src.scraping_helpers import (\n",
    "    get_guardian_text_from_soup,\n",
    "    get_hubble_text_from_soup,\n",
    "    get_space_text_from_soup,\n",
    "    get_nytimes_text_from_soup,\n",
    "    save_dflist_hdfs,\n",
    "    append_datetime_attrs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Scrape articles](#scrape-articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use files of URLs to listings retrieved in `1_get_list_of_urls.ipynb` and scrape the text of these news articles (from various publications) and store the retrieved text data in `data/raw/<publication-name>.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# General inputs\n",
    "data_dir = str(Path().cwd() / \"data\" / \"raw\")\n",
    "min_delay_between_scraped = 0\n",
    "max_delay_between_scraped = 1\n",
    "\n",
    "# Paths to url files and dictionaries of lists of urls\n",
    "list_of_urls_file = {\n",
    "    \"space\": Path(data_dir) / \"space_com_urls.csv\",\n",
    "    \"guardian\": Path(data_dir) / \"guardian_urls.csv\",\n",
    "    \"hubble\": Path(data_dir) / \"hubble_urls.csv\",\n",
    "    \"nytimes\": Path(data_dir) / \"nytimes_urls__*.csv\",\n",
    "}\n",
    "\n",
    "# Dictionary of lists of urls, for all publications\n",
    "urls = {\n",
    "    \"guardian\": pd.read_csv(list_of_urls_file[\"guardian\"])[\"webUrl\"].tolist(),\n",
    "    \"hubble\": pd.read_csv(list_of_urls_file[\"hubble\"])[\"url\"].tolist(),\n",
    "    \"space\": pd.read_csv(list_of_urls_file[\"space\"])[\"url\"].tolist(),\n",
    "    \"nytimes\": pd.concat(\n",
    "        [pd.read_csv(f) for f in glob(str(list_of_urls_file[\"nytimes\"]))],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )[\"web_url\"].tolist(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of urls, by publication, used during development to manually test scraping/utility code\n",
    "guardian_urls = [\n",
    "    \"https://www.theguardian.com/science/1986/jan/29/spaceexploration.columbia\",\n",
    "    \"https://www.theguardian.com/science/1969/jul/21/spaceexploration.archive1\",\n",
    "    \"https://www.theguardian.com/science/1999/jul/18/spaceexploration.theobserver\",\n",
    "    \"https://www.theguardian.com/science/1999/aug/11/eclipse.uknews4\",\n",
    "    \"https://www.theguardian.com/science/1999/aug/05/technology1\",\n",
    "    \"https://www.theguardian.com/science/1999/jul/23/eclipse\",\n",
    "    \"https://www.theguardian.com/science/1986/jan/30/spaceexploration.columbia\",\n",
    "    \"https://www.theguardian.com/science/1986/jan/30/spaceexploration.columbia1\",\n",
    "    \"https://www.theguardian.com/science/2014/nov/10/breakthrough-prize-scientists-23m-science-awards-2015\",\n",
    "    \"https://www.theguardian.com/technology/2015/jan/16/elon-musk-falcon-9-rapid-unscheduled-disassembly\",\n",
    "    \"https://www.theguardian.com/science/2014/oct/31/-sp-rosetta-selfie-mars-saturn-a-month-in-space-september-2014\",\n",
    "    \"https://www.theguardian.com/science/blog/2014/oct/31/pumpkins-halloween-yeti-mental-health-polio-blogs-roundup\",\n",
    "    \"https://www.theguardian.com/science/2014/nov/09/steven-pinker-twitter-can-hone-writing-skills\",\n",
    "    \"https://www.theguardian.com/science/2014/nov/12/rosetta-mission-philae-historic-landing-comet\",\n",
    "]\n",
    "hubble_urls = [\n",
    "    \"https://webbtelescope.org/contents/news-releases/2019/news-2019-41\",\n",
    "    \"https://hubblesite.org/contents/news-releases/2018/news-2018-21.html\",\n",
    "    \"https://hubblesite.org/contents/news-releases/2018/news-2018-03.html\",\n",
    "    \"https://hubblesite.org/contents/news-releases/1990/news-1990-23.html\",\n",
    "    \"https://hubblesite.org/contents/news-releases/1990/news-1990-17.html\",\n",
    "]\n",
    "space_urls = [\n",
    "    \"https://www.space.com/7078-shuttle-astronauts-deploy-satellites-landing.html\",\n",
    "    \"https://www.space.com/201-explore-colors-stars.html\",\n",
    "    \"https://www.space.com/9429-cargo-ship-delivers-healthy-halloween-treats-space-station.html\",\n",
    "    \"https://www.space.com/37172-trappist-1-planet-visualizations-explained.html\",\n",
    "    \"https://www.space.com/15107-venus-pleiades-april-skywatching-tips.html\",\n",
    "    \"https://www.space.com/11521-royal-wedding-space-station-astronauts-message.html\",\n",
    "]\n",
    "nytimes_urls = [\n",
    "    \"https://www.nytimes.com/2019/11/09/us/politics/impeachment-state-department.html?action=click&module=Top%20Stories&pgtype=Homepage\",\n",
    "    \"http://www.nytimes.com/2015/01/02/world/europe/turkey-police-thwart-attack-on-prime-ministers-office.html\",\n",
    "    \"https://www.nytimes.com/2019/11/09/us/politics/michael-bloomberg-democrats.html?action=click&module=Top%20Stories&pgtype=Homepage\",\n",
    "    \"https://www.nytimes.com/2019/11/04/science/space/nasa-boeing-starliner-tes.html\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scrape-articles\"></a>\n",
    "\n",
    "## 2. [Scrape articles](#scrape-articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  ========= GET SOUP FOR A SINGLE URL (DO NOT DELETE) =========\n",
    "# link =\n",
    "# page_response = requests.get(link, timeout=5)\n",
    "# soup = BeautifulSoup(page_response.content, \"lxml\")\n",
    "# print(soup.prettify())\n",
    "# text, date = get_space_text_from_soup(soup, page_response)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  ========= DATES FROM SPACE.com (DO NOT DELETE) =========\n",
    "# dates = []\n",
    "# for url in space_urls:\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content, \"lxml\")\n",
    "#     # print(soup.prettify())\n",
    "#     published_datetime = soup.find(\"meta\", {\"name\": \"pub_date\"}).get(\"content\")\n",
    "#     dates.append(published_datetime)\n",
    "# # print(dates)\n",
    "# df = pd.DataFrame(pd.Series(dates), columns=[\"publication_date\"])\n",
    "# df = append_datetime_attrs(df, date_col=\"publication_date\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========= APPEND SINGLE ARTICLE SCRAPE TO HDF5 FILE (DO NOT DELETE) =========\n",
    "# l = []\n",
    "# for site, links in urls.items():\n",
    "#     # print(site)\n",
    "#     for k, link in enumerate(links[:2]):\n",
    "#         # print(f\"{site}_{k+1}\")\n",
    "#         df_row = pd.DataFrame(np.random.rand(1, 9), columns=list(\"ABCDEFGHI\"))\n",
    "#         df_row[\"publication\"] = site\n",
    "#         df_row.to_hdf(h5_path, key=f\"{site}_{k+1}\", format=\"t\", mode=\"a\")\n",
    "#         l.append(df_row)\n",
    "# print(f\"Scraped {len(l)} articles\")\n",
    "# pd.concat(\n",
    "#     [\n",
    "#         pd.read_hdf(h5_path, key=f\"{site}_{k+1}\")\n",
    "#         for site in urls.keys()\n",
    "#         for k, link in enumerate(links[:2])\n",
    "#     ],\n",
    "#     axis=0,\n",
    "#     ignore_index=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will iterate over a Python dictionary of all the news publications and perform the following actions within each iteration\n",
    "1. Scrape page with `BeautifulSoup` and get soup\n",
    "2. Get text and (optionally) date from soup\n",
    "3. Store extracted contents from soup in a dictionary\n",
    "4. Convert dictionary into single row `DataFrame`\n",
    "5. Rename date column\n",
    "6. Apend publication name as column to `DataFrame`\n",
    "7. Export single-row `DataFrame` to HDF file\n",
    "   - this would be a single listing's details\n",
    "8. Convert dictionary with all rows into `DataFrame`\n",
    "   - this would be all listings' details\n",
    "9. Append datetime attributes to full `DataFrame`\n",
    "10. Append full `DataFrame` for publication to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Main controller loop for scraping article text from url\n",
    "l = []\n",
    "for site, links in urls.items():\n",
    "    (Path(data_dir) / site).mkdir(parents=True, exist_ok=True)\n",
    "    # print(site, links)\n",
    "    l_texts = {}\n",
    "    date_published = np.nan\n",
    "    for k, link in enumerate(links[30000:]):\n",
    "        l_texts_single_listing = {}\n",
    "        print(f\"Scraping article number {k+1} from {site}, Link: {link}\")\n",
    "        # print(site, link)\n",
    "        start_time = time()\n",
    "        try:\n",
    "            # 1. Get soup\n",
    "            page_response = requests.get(link, timeout=5)\n",
    "            soup = BeautifulSoup(page_response.content, \"lxml\")\n",
    "            # print(soup.prettify())\n",
    "            # 2. Get text (and optionally date)\n",
    "            if site == \"guardian\":\n",
    "                text = get_guardian_text_from_soup(soup)\n",
    "            elif site == \"hubble\":\n",
    "                text = get_hubble_text_from_soup(soup)\n",
    "            elif site == \"space\":\n",
    "                text, date_published = get_space_text_from_soup(soup, page_response)\n",
    "            elif site == \"nytimes\":\n",
    "                text, date_published = get_nytimes_text_from_soup(soup)\n",
    "        except Exception as e:\n",
    "            print(f\"Experienced error {str(e)} when scraping {link} from {site}\")\n",
    "            text = np.nan\n",
    "        scrape_minutes, scrape_seconds = divmod(time() - start_time, 60)\n",
    "        print(\n",
    "            f\"Scraping time: {int(scrape_minutes):d} minutes, {scrape_seconds:.2f} seconds\"\n",
    "        )\n",
    "        \n",
    "        # 3. Store text and date in dictionary\n",
    "        l_texts[link] = [text, date_published]\n",
    "        l_texts_single_listing[link] = [text, date_published]\n",
    "        # 4. Convert dictionary of text and date, for single listing, to DataFrame\n",
    "        df_row = pd.DataFrame.from_dict(\n",
    "            l_texts_single_listing, orient=\"index\"\n",
    "        ).reset_index()\n",
    "        # 5. Rename publication date column of DataFrame\n",
    "        df_row.rename(\n",
    "            columns={\"index\": \"url\", 0: \"text\", 1: \"publication_date\"}, inplace=True\n",
    "        )\n",
    "        # 6. Append publication name as column to DataFrame\n",
    "        df_row[\"publication\"] = site\n",
    "        # print(Path(data_dir) / site / f\"scrapes_{site}_{k+1}.h5\")\n",
    "        # 7. Store DataFrame in HDF file\n",
    "        df_row.to_hdf(\n",
    "            Path(data_dir) / site / f\"scrapes_{site}_{k+1}.h5\",\n",
    "            key=f\"{site}_{k+1}\",\n",
    "            format=\"t\",\n",
    "            mode=\"w\",\n",
    "        )\n",
    "        # print(text)\n",
    "        # Delay between scraping urls\n",
    "        delay_between_scrapes = randint(\n",
    "            min_delay_between_scraped, max_delay_between_scraped\n",
    "        )\n",
    "        if (k + 1) < len(links[30000:]):\n",
    "            print(f\"Pausing for {delay_between_scrapes} seconds\\n\")\n",
    "            sleep(delay_between_scrapes)\n",
    "    \n",
    "    # 8. Convert dictionary of text and date, for all listings, to DataFrame\n",
    "    df = pd.DataFrame.from_dict(l_texts, orient=\"index\").reset_index()\n",
    "    df.rename(columns={\"index\": \"url\", 0: \"text\", 1: \"publication_date\"}, inplace=True)\n",
    "    df[\"publication\"] = site\n",
    "    # display(df)\n",
    "    # 9. (Optional) Append datetime attributes for space.com and nytimes publications\n",
    "    if site in [\"space\", \"nytimes\"]:\n",
    "        df = append_datetime_attrs(df, date_col=\"publication_date\", publication=site)\n",
    "    else:\n",
    "        for L in [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"dayofweek\",\n",
    "            \"dayofyear\",\n",
    "            \"weekofyear\",\n",
    "            \"quarter\",\n",
    "        ]:\n",
    "            df[L] = np.nan\n",
    "    # 10. Append DataFrame to list\n",
    "    l.append(df)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========= LOAD SUBSET (NOT ALL) OF HDF5 FILES (DO NOT DELETE) =========\n",
    "# pd.concat(\n",
    "#     [\n",
    "#         pd.read_hdf(Path(data_dir) / site / f\"scrapes_{site}_{k+1}.h5\", key=f\"{site}_{k+1}\")\n",
    "#         for site, links in urls.items()\n",
    "#         for k, link in enumerate(links[0:10])\n",
    "#     ],\n",
    "#     axis=0,\n",
    "#     ignore_index=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll concatenate the list of `DataFrame`s of scraped text data into a single `DataFrame` and export it to a separate `*.csv` file per publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.concat(l, axis=0, ignore_index=True).drop_duplicates()\n",
    "if site == \"space\":\n",
    "    dfs = dfs[~pd.isnull(dfs[\"text\"])]\n",
    "display(dfs)\n",
    "print(dfs.shape)\n",
    "dfs.to_csv(Path(data_dir) / f\"{site}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
