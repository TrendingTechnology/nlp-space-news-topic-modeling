{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NLP trials](#nlp-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import math\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from time import time\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dask.distributed import Client\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.pipe_helpers\n",
    "from src.pipe_helpers import (\n",
    "    TextCleaner,\n",
    "    StemTokenizer,\n",
    "    NLTKLemmaTokenizer,\n",
    "    SpacyLemmaTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import (\n",
    "    plot_horiz_bar,\n",
    "    pipe_get_topics,\n",
    "    get_main_topic_percentage,\n",
    "    altair_datetime_heatmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 26\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "alt.renderers.enable(\"notebook\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load joined data](#load-joined-data)\n",
    "3. [Create analysis pipeline](#create-analysis-pipeline)\n",
    "4. [Use pipeline to retrieve topics](#use-pipeline-to-retrieve-topics)\n",
    "5. [Exploring topics combined with source data](#exploring-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with NLP models on the joined news listings data in `data/processed/*_processed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "publication_name = \"guardian\"\n",
    "\n",
    "# Data locations\n",
    "data_dir_path = str(\n",
    "    Path().cwd() / \"data\" / \"processed\" / f\"{publication_name}_processed.csv\"\n",
    ")\n",
    "\n",
    "# Custom stop words to include\n",
    "manual_stop_words = [\"nt\", \"ll\", \"ve\"]\n",
    "\n",
    "# Topic naming\n",
    "mapping_dict = {\n",
    "    \"nytimes\": {\n",
    "        \"component_1\": \"Academia\",\n",
    "        \"component_2\": \"Shuttle Missions and Crashes\",\n",
    "        \"component_3\": \"Digital\",\n",
    "        \"component_4\": \"Mars Exploration\",\n",
    "        \"component_5\": \"Imaging Stars - Astronomy\",\n",
    "        \"component_6\": \"Rocket Launches - Testing and Moon Landing\",\n",
    "        \"component_7\": \"Dark Matter theories\",\n",
    "        \"component_8\": \"Planetary Research\",\n",
    "        \"component_9\": \"Space Funding Bodies\",\n",
    "        \"component_10\": \"ISS - USA and Russian segments\",\n",
    "        \"component_11\": \"Gravity and Black Holes - Hawking\",\n",
    "        \"component_12\": \"Global Warming\",\n",
    "        \"component_13\": \"Studying Comets and Meteors\",\n",
    "        \"component_14\": \"Soviet Union Spy Satellites\",\n",
    "        \"component_15\": \"Discover of Sub-Atomic particles\",\n",
    "    },\n",
    "    \"guardian\": {\n",
    "        \"component_1\": \"Academia\",\n",
    "        \"component_2\": \"ISS - USA and Russian segments\",\n",
    "        \"component_3\": \"Mars Exploration\",\n",
    "        \"component_4\": \"Imaging Stars - Astronomy\",\n",
    "        \"component_5\": \"Studying Comets and Meteors\",\n",
    "        \"component_6\": \"Discover of Sub-Atomic particles\",\n",
    "        \"component_7\": \"Rocket Launches - Moon Landing\",\n",
    "        \"component_8\": \"Shuttle Missions and Crashes\",\n",
    "        \"component_9\": \"Saturn Research\",\n",
    "        \"component_10\": \"Space Funding Bodies\",\n",
    "        \"component_11\": \"Objects crashing into Earth\",\n",
    "        \"component_12\": \"Gravity and Black Holes - Hawking\",\n",
    "        \"component_13\": \"Rocket Launches - Testing\",\n",
    "        \"component_14\": \"Planetary Research\",\n",
    "        \"component_15\": \"Global Warming\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# General inputs\n",
    "n_topics_wanted = 15\n",
    "number_of_words_per_topic_to_show = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words from all packages\n",
    "# # NLTK\n",
    "if not ((Path.cwd().parents[1]) / \"nltk_data\").exists():\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "else:\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "# Spacy and sklearn\n",
    "spacy_stop_words = STOP_WORDS\n",
    "sklearn_stop_words = stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Assemble manual list of stop words\n",
    "spacy_not_in_sklearn = set(spacy_stop_words) - set(sklearn_stop_words)\n",
    "nltk_not_in_sklearn = set(nltk_stop_words) - set(sklearn_stop_words)\n",
    "all_stop_words = set(\n",
    "    list(set(sklearn_stop_words))\n",
    "    + list(spacy_not_in_sklearn)\n",
    "    + list(nltk_not_in_sklearn)\n",
    ")\n",
    "\n",
    "# Manually add to stop words\n",
    "for manual_stop_word in manual_stop_words:\n",
    "    all_stop_words.add(manual_stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-joined-data\"></a>\n",
    "\n",
    "## 2. [Load joined data](#load-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the joined data from from a publication, stored at `data/processed/<publication-name>_processed.csv`, into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(data_dir_path))\n",
    "df = df[[\"text\", \"year\"]]\n",
    "print(df.shape[0])\n",
    "display(df.head())\n",
    "# print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.loc[:, \"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define a custom index to be used when defining the `DataFrame` of topics and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_label = [t[:30] + \"...\" for t in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-analysis-pipeline\"></a>\n",
    "\n",
    "## 3. [Create analysis pipeline](#create-analysis-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the various components of the NLP analysis pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vectorizer, we will instantiate 3 options for tokenization\n",
    "- `lemmatizer_vectorizer`\n",
    "  - this will perform lemmatization using either the [NLTK module](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/) or [Spacy](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
    "- `stemmer_vectorizer`\n",
    "  - this will perform stemming using [NLTK](http://www.nltk.org/howto/stem.html)\n",
    "- `vectorizer`\n",
    "  - this will use the string tokenization built in to [`scikit-learn`'s `TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=NLTKLemmaTokenizer(\n",
    "        stopwords=all_stop_words, clean=True, split=True, get_word_pos=True\n",
    "    ),  # \"all_stop_words\" or \"english\"\n",
    "    # tokenizer=SpacyLemmaTokenizer(stopwords=all_stop_words, clean=True, split=True),  # \"all_stop_words\" or \"english\"\n",
    "    lowercase=True,\n",
    "    strip_accents=\"ascii\",\n",
    "    token_pattern=\"[a-z][a-z]+\",\n",
    ")\n",
    "stemmer_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=StemTokenizer(\n",
    "        stype=\"snowball\", stopwords=all_stop_words, clean=True, split=True\n",
    "    ),  # \"all_stop_words\" or \"english\"\n",
    "    lowercase=True,\n",
    "    strip_accents=\"ascii\",\n",
    "    token_pattern=\"[a-z][a-z]+\",\n",
    ")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    stop_words=all_stop_words,  # \"all_stop_words\" or \"english\"\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    binary=False,\n",
    "    strip_accents=\"ascii\",\n",
    "    token_pattern=\"[a-z][a-z]+\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will put these together into a `scikit-learn` Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline without lemmatization or stemming\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"cleaner\", TextCleaner(split=False)),\n",
    "        (\"vectorizer\", vectorizer,),\n",
    "        (\"nmf\", NMF(n_components=n_topics_wanted)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# # Pipeline with lemmatization or stemming\n",
    "# pipe = Pipeline(\n",
    "#     steps=[\n",
    "#         (\"vectorizer\", stemmer_vectorizer),  # with stemming and no lemmatization\n",
    "#         # (\"vectorizer\", lemmatizer_vectorizer),  # with lemmatization and no stemming\n",
    "#         (\"nmf\", NMF(n_components=n_topics_wanted)),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use-pipeline-to-retrieve-topics\"></a>\n",
    "\n",
    "## 4. [Use pipeline to retrieve topics](#use-pipeline-to-retrieve-topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the NLP analysis pipeline to retrieve topics from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 3 options to combine the assigned topics to the source `DataFrame` of documents\n",
    "1. [Get](https://stackoverflow.com/a/38955314/4057186) top n-highest value columns, **sorted** by (highest) value, for each row\n",
    "2. Get top-n highest value columns by threshold value (i.e. all those above a threshold value)\n",
    "   - least prefered option because cutoff threshold is arbitrarily chosen\n",
    "3. Get top-1 highest value column for each row\n",
    "   - this was the chosen option for assigning values to the `all_topics` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a `scikit-learn` pipeline, We'll use parallelize the operation using [`dask` with `joblib`](https://examples.dask.org/machine-learning.html#Distributed-Training). We will instantiate a `dask` local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False, threads_per_worker=4, n_workers=1, memory_limit=\"3GB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a helper function to\n",
    "- retrieve two `DataFrame`s that contain the topics\n",
    "- generate a plot of topics and top 10 words (by how much each document is made up of the resulting topics) for each topic\n",
    "- replace placeholder topic names by custom topic names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    df_with_topics, df_source_with_topics, topic_word, fig = pipe_get_topics(\n",
    "        corpus,\n",
    "        pipe=pipe,\n",
    "        n_topics_wanted=n_topics_wanted,\n",
    "        df=df,\n",
    "        y_tick_mapper_list=list(mapping_dict[publication_name].values()),\n",
    "        vt_df_index=None,\n",
    "        analysis_type=\"nmf\",\n",
    "        number_of_words_per_topic_to_show=number_of_words_per_topic_to_show,\n",
    "        show_bar_labels=False,\n",
    "    )\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll close the `dask` cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "fig.savefig(\n",
    "    Path().cwd() / \"reports\" / \"figures\" / f\"{publication_name}_nmf_topics.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_with_topics.head(2))\n",
    "display(df_source_with_topics.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll show a summary `DataFrame` that summarizes the average number of times a topic was chosen as the most popular topic per `year`\n",
    "- this will not be used in subsequent analysis but shows a summary of the topics by `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_percentages = get_main_topic_percentage(\n",
    "    df_with_topics.drop([\"document\"], axis=1),\n",
    "    group_by_col=\"year\",\n",
    "    quant_col_idx_start=1,\n",
    ")\n",
    "df_percentages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-topics-combined-with-source-data\"></a>\n",
    "\n",
    "## 5. [Exploring topics combined with source data](#exploring-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show a heatmap of the most popular topic by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df_with_topics.groupby([\"most_popular_topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"most_popular_topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "topics_by_timeframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"most_popular_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Most popular topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\n",
    "            \"title\": \"Most popular topic\",\n",
    "            \"field\": \"most_popular_topic\",\n",
    "            \"type\": \"nominal\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as main topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=Path().cwd() / \"reports\" / \"figures\" / \"my_heatmap.html\",\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of occurrences of the `\"Space Funding Bodies\"` as the most popular topic, relative to the year 1980\n",
    "- this will approximate the public interest in changes in this topic over the years investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = (\n",
    "    topics_by_timeframe[\n",
    "        topics_by_timeframe[\"most_popular_topic\"] == \"Space Funding Bodies\"\n",
    "    ]\n",
    "    .set_index(\"year\")[\"count\"]\n",
    "    .sort_index()\n",
    ")\n",
    "funds / funds.loc[funds.index.min()]\n",
    "funds = funds / funds.loc[funds.index.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "funds.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Cyclic variation in funding as main topic in article\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Funding\\n(rel. to 1981)\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of words in the `\"text\"` of the article\n",
    "- this will approximate the length of articles over the years investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_chars\"] = df[\"text\"].str.split().str.len()\n",
    "word_count = df[[\"article_chars\", \"year\"]].groupby([\"year\"])[\"article_chars\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "word_count.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Words in article\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Word count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "df[\"year\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8\n",
    ")\n",
    "ax.set_title(\n",
    "    \"Articles per year\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Article count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "pd.DataFrame(word_count).merge(\n",
    "    pd.DataFrame(df[\"year\"].value_counts()),\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").plot.scatter(x=\"year\", y=\"article_chars\", ax=ax)\n",
    "ax.set_title(\n",
    "    \"Word count increases with number of articles\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"Articles per year\", fontweight=\"bold\")\n",
    "h = plt.ylabel(\"Word count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the New York Times, between 1981 and 2004, article length was fairly consistent. There was a significant drop in the years 2005 and 2012. Since 2013, article length has been steadily increasing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
