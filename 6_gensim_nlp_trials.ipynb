{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Gensim NLP trials](#gensim-nlp-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "\n",
    "import altair as alt\n",
    "import gensim.corpora as corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim.models import nmf, Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.pipe_helpers\n",
    "from src.pipe_helpers import TextCleaner\n",
    "\n",
    "%aimport src.gensim_helpers\n",
    "from src.gensim_helpers import (\n",
    "    compute_coherence_values,\n",
    "    make_bigrams,\n",
    "    remove_stopwords,\n",
    "    lemmatization,\n",
    "    sent_to_words,\n",
    "    format_topics_sentences,\n",
    "    get_bigrams_trigrams,\n",
    "    plot_coherence_scores,\n",
    "    compute_coherence_values,\n",
    ")\n",
    "\n",
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import (\n",
    "    altair_datetime_heatmap,\n",
    "    plot_horiz_bar,\n",
    "    plot_horiz_bar_gensim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 26\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load joined data](#load-joined-data)\n",
    "3. [Topic modeling using Gensim NMF with TFIDF vectorization](#topic-modeling-using-gensim-nmf-with-tfidf-vectorization)\n",
    "4. [Topic modeling using Gensim NMF without TFIDF vectorization](#topic-modeling-using-gensim-nmf-without-tfidf-vectorization)\n",
    "   - 4.1. [Pre-processing for Gensim NMF](#pre-processing-for-gensim-nmf)\n",
    "   - 4.2. [Gensim NMF](#gensim-nmf)\n",
    "   - 4.3. [Exploring Gensim NMF topics combined with source data](#exploring-gensim-nmf-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with NLP models on the joined news listings data in `data/processed/*_processed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "publication_name = \"guardian\"\n",
    "\n",
    "# Data locations\n",
    "data_dir_path = str(\n",
    "    Path().cwd() / \"data\" / \"processed\" / f\"{publication_name}_processed.csv\"\n",
    ")\n",
    "cloud_run = True\n",
    "\n",
    "# Custom stop words to include\n",
    "manual_stop_words = [\"nt\", \"ll\", \"ve\"]\n",
    "\n",
    "# Topic naming\n",
    "gensim_non_tfidf_mapping_dict = {\n",
    "    \"guardian\": {\n",
    "        0: \"Studying Comets and Meteors\",  #\n",
    "        1: \"Rocket Launches - Testing\",  #\n",
    "        2: \"Discover of Sub-Atomic particles\",  #\n",
    "        3: \"Learning and Memory\",  #\n",
    "        4: \"ISS\",  #\n",
    "        5: \"Brain Research\",  #\n",
    "        6: \"Academia\",  ##\n",
    "        7: \"Rocket Launches - Moon Landing\",  #\n",
    "        8: \"Pseudo space-science and Humanity - Opinion\",  #\n",
    "        9: \"Imaging Stars - Astronomy\",  #\n",
    "        10: \"Planetary Research\",  #\n",
    "        11: \"Global Warming and Climate Science\",  #\n",
    "        12: \"Dark Matter Theories\",  ##\n",
    "        13: \"Space Funding Bodies\",  ##\n",
    "        14: \"Mars Exploration\",  #\n",
    "    }\n",
    "}\n",
    "\n",
    "# General inputs\n",
    "limit = 25\n",
    "start = 10\n",
    "step = 1\n",
    "n_top_words = 10\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(w2v_model, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(w2v_model.wv.similarity(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "\n",
    "def get_descriptor(all_terms, H, topic_index, top):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(all_terms[term_index])\n",
    "    return top_terms\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    def __init__(self, documents, stopwords):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(\"Building Word2Vec model ...\")\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall(doc):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append(\"<stopword>\")\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append(tok)\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "def fit_nmf_for_num_topics(\n",
    "    start: int, limit: int, random_state: int, doc_term_matrix\n",
    ") -> list:\n",
    "    topic_models = []\n",
    "    for num_topics in range(start, limit + 1):\n",
    "        print(f\" > Applying NMF with {num_topics:0d} topics...\", end=\"\")\n",
    "        model = NMF(n_components=num_topics, max_iter=700, random_state=random_state)\n",
    "        model_transformed = model.fit_transform(docs_terms)\n",
    "        print(\"done\")\n",
    "        factors_dict = model.components_\n",
    "        topic_models.append((num_topics, model_transformed, factors_dict))\n",
    "    return topic_models\n",
    "\n",
    "\n",
    "def compute_coherence_values_manually(\n",
    "    topic_models_fitted: list, n_top_words: int, word2vec_model_fitted: Word2Vec\n",
    ") -> None:\n",
    "    k_values = []\n",
    "    coherences = []\n",
    "    for (k, W, H) in topic_models_fitted:\n",
    "        # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "        term_rankings = []\n",
    "        for topic_index in range(k):\n",
    "            term_rankings.append(get_descriptor(terms, H, topic_index, n_top_words))\n",
    "        # Now calculate the coherence based on our Word2vec model\n",
    "        k_values.append(k)\n",
    "        coherences.append(calculate_coherence(word2vec_model_fitted, term_rankings))\n",
    "        print(f\" > Applied NMF for k={k:0d} and found coherence={coherences[-1]:.4f}\")\n",
    "\n",
    "\n",
    "def print_top_words(\n",
    "    topic_models: list,\n",
    "    num_topics: int,\n",
    "    n_top_words: int,\n",
    "    start: int,\n",
    "    feature_names: list,\n",
    "    docs_terms,\n",
    "    method: int = 2,\n",
    "    random_state: int = 42,\n",
    ") -> None:\n",
    "    print(f\"Top terms per topic, using random_state={random_state}:\")\n",
    "    if method == 1:\n",
    "        # get the model that we generated earlier.\n",
    "        W, H = topic_models[num_topics - start][1:]\n",
    "        for topic_index in range(n_top_words):\n",
    "            descriptor = get_descriptor(terms, H, topic_index, n_top_words)\n",
    "            str_descriptor = \" \".join(descriptor)\n",
    "            print(f\"Topic {topic_index+1:0d}: {str_descriptor}\")\n",
    "    else:\n",
    "        best_model = NMF(\n",
    "            n_components=num_topics, max_iter=700, random_state=random_state\n",
    "        ).fit(docs_terms)\n",
    "        for topic_idx, topic in enumerate(best_model.components_):\n",
    "            message = f\"Topic {topic_idx:0d}: \"\n",
    "            message += \" \".join(\n",
    "                [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n",
    "            )\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words from all packages\n",
    "# NLTK\n",
    "if not ((Path.cwd().parents[1]) / \"nltk_data\").exists():\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "# Spacy and sklearn\n",
    "spacy_stop_words = STOP_WORDS\n",
    "sklearn_stop_words = stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Assemble manual list of stop words\n",
    "spacy_not_in_sklearn = set(spacy_stop_words) - set(sklearn_stop_words)\n",
    "nltk_not_in_sklearn = set(nltk_stop_words) - set(sklearn_stop_words)\n",
    "all_stop_words = set(\n",
    "    list(set(sklearn_stop_words))\n",
    "    + list(spacy_not_in_sklearn)\n",
    "    + list(nltk_not_in_sklearn)\n",
    ")\n",
    "\n",
    "# Manually add to stop words\n",
    "for manual_stop_word in manual_stop_words:\n",
    "    all_stop_words.add(manual_stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-joined-data\"></a>\n",
    "\n",
    "## 2. [Load joined data](#load-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the joined data from from a publication, stored at `data/processed/<publication-name>_processed.csv`, into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(data_dir_path))\n",
    "df = df[[\"text\", \"year\"]]\n",
    "print(df.shape[0])\n",
    "display(df.head())\n",
    "# print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = df.loc[:, \"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words=all_stop_words,\n",
    "    min_df=20,\n",
    "    max_features=None,\n",
    "    binary=False,\n",
    "    strip_accents=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "docs_terms = vectorizer.fit_transform(corpus_raw)\n",
    "print(\n",
    "    f\"Created {docs_terms.shape[0]:0d} X {docs_terms.shape[1]:0d} \"\n",
    "    \"TF-IDF-normalized document-term matrix\"\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(f\"Vocabulary has {len(terms):0d} distinct terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "topic_models = fit_nmf_for_num_topics(start, limit, random_state, docs_terms)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "docgen = TokenGenerator(corpus_raw, all_stop_words)\n",
    "w2v_model = Word2Vec(docgen, size=500, min_count=20, sg=1, seed=1)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model has {len(w2v_model.wv.vocab):0d} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "compute_coherence_values_manually(topic_models, n_top_words, Word2Vec)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence_scores(coherences, start, limit, step, (12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_topics = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(\n",
    "    topic_models,\n",
    "    best_n_topics,\n",
    "    n_top_words,\n",
    "    start,\n",
    "    terms,\n",
    "    docs_terms,\n",
    "    method=2,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Topic 01: people, says, brain, like, life, time, human, world, think, species\n",
    "Topic 02: rocket, spacex, space, launch, musk, company, falcon, flight, satellite, satellites\n",
    "Topic 03: mars, beagle, martian, planet, rover, life, water, mission, lander, nasa\n",
    "Topic 04: planets, planet, star, earth, stars, sun, kepler, life, solar, astronomers\n",
    "Topic 05: comet, rosetta, philae, lander, comets, 67p, sun, dust, mission, surface\n",
    "Topic 06: telescope, hubble, stars, galaxies, universe, galaxy, light, dark, astronomers, matter\n",
    "Topic 07: moon, lunar, apollo, armstrong, space, nasa, china, astronauts, surface, earth\n",
    "Topic 08: shuttle, nasa, columbia, discovery, launch, space, astronauts, mission, station, flight\n",
    "Topic 09: asteroid, asteroids, earth, impact, rock, object, collision, hit, near, orbit\n",
    "Topic 10: station, space, mir, russian, iss, crew, peake, astronaut, russia, international\n",
    "Topic 11: cassini, saturn, titan, huygens, rings, moons, enceladus, spacecraft, probe, moon\n",
    "Topic 12: science, uk, research, climate, space, government, said, scientific, scientists, public\n",
    "Topic 13: higgs, particle, particles, lhc, matter, physics, dark, cern, energy, boson\n",
    "Topic 14: black, hawking, gravitational, universe, waves, theory, holes, einstein, hole, relativity\n",
    "Topic 15: pluto, horizons, planet, dwarf, solar, new, kuiper, stern, belt, charon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-modeling-using-gensim-nmf-without-tfidf-vectorization\"></a>\n",
    "\n",
    "## 4. [Topic modeling using Gensim NMF without TFIDF vectorization](#topic-modeling-using-gensim-nmf-without-tfidf-vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use Gensim's implementation of NMF, without TFIDF vectorization, to retrieve topics. This will be done without TFIDF Vectorization from either [`sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer) or [`gensim`](https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel) itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing-for-gensim-nmf\"></a>\n",
    "\n",
    "### 4.1. [Pre-processing for Gensim NMF](#pre-processing-for-gensim-nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll clean the text of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"cleaner\", TextCleaner(split=False))])\n",
    "corpus_raw_cleaned = pipe.fit_transform(corpus_raw)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now tokenize the cleaned sentences into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# data_words = list(sent_to_words(corpus_raw))\n",
    "data_words = list(sent_to_words(corpus_raw_cleaned))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Gensim's `Phrases` module to build bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "bigram_model, trigram_model = get_bigrams_trigrams(data_words, 5, 100)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform the following pre-processing\n",
    "- remove stopwords\n",
    "- (optional) create bigrams\n",
    "- (optional) lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words, all_stop_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops, bigram_model)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_nostops, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a corpus comprising an assigned ID and corresponding frequency of words from the cleaned list of words (where stopwords were removed) above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word = corpora.Dictionary(data_words_nostops)\n",
    "\n",
    "# Term Document Frequency for corpus\n",
    "# corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "corpus = [id2word.doc2bow(text) for text in data_words_nostops]\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gensim-nmf\"></a>\n",
    "\n",
    "### 4.2. [Gensim NMF](#gensim-nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train Gensim's NMF model. A helper function below will iterate over the number of topics and compute the coherence score for each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "model_dict, coherence_values = compute_coherence_values(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    # texts=data_lemmatized,\n",
    "    texts=data_words_nostops,\n",
    "    limit=limit,\n",
    "    start=start,\n",
    "    step=step,\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coherence scores are graphed below by number of topics used, [with an annotation](https://matplotlib.org/2.0.0/users/annotations.html#annotating-with-text-with-box) showing the number of topics with the highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence_scores(coherence_values, start, limit, step, (12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. There isn't strong evidence of an increasing trend in the number of topics, nor of a plateau in the scores. While Gensim model hyperparameter optimization could be further explored, the scores do not show a preference for a specific choice of number of topics over the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 15 topics for further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "best_model = nmf.Nmf(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=model_dict[15].num_topics,\n",
    "    chunksize=len(corpus),  # no. of docs to be used in each training chunk\n",
    "    passes=10,\n",
    "    kappa=1.0,\n",
    "    minimum_probability=0.01,\n",
    "    w_max_iter=200,\n",
    "    w_stop_condition=0.0001,\n",
    "    h_max_iter=50,\n",
    "    h_stop_condition=0.001,\n",
    "    eval_every=10,\n",
    "    normalize=True,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll print out all the topics found from the Gensim NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twords = {}\n",
    "print(f\"Top terms per topic, using random_state={random_state}:\")\n",
    "for topic, word in best_model.show_topics(\n",
    "    num_topics=len(gensim_non_tfidf_mapping_dict[publication_name]), num_words=10\n",
    "):\n",
    "    words_cleaned = re.sub(\"[^A-Za-z ]+\", \"\", word)\n",
    "    twords[topic] = words_cleaned\n",
    "    print(f\"Topic {topic}:\", words_cleaned.replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics and their top ten words are shown below, for the pre-determined choice of `random_state` of `42` specified in the Gensim NMF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Top terms per cluster, using random_state=42:\n",
    "Topic 0: life time earth comet years scientists like species book dna\n",
    "Topic 1: space earth spacecraft satellites orbit satellite mission solar rocket moon\n",
    "Topic 2: particles universe particle theory physics higgs energy lhc matter time\n",
    "Topic 3: people memory like memories price day dont think remember things\n",
    "Topic 4: space station nasa shuttle astronauts launch mission said flight crew\n",
    "Topic 5: brain cells stem human tissue body cell neurons embryonic brains\n",
    "Topic 6: number atomic species new matter xenon human tellurium dark numbers\n",
    "Topic 7: says moon people like time lunar world going apollo think\n",
    "Topic 8: said time dawkins think people human like new dont world\n",
    "Topic 9: stars telescope star said light black astronomers galaxy years universe\n",
    "Topic 10: planet planets solar earth water sun scientists surface pluto ice\n",
    "Topic 11: work ice theory equation space climate surface time mathematical moduli\n",
    "Topic 12: research says new university scientists work matter scientific uk dark\n",
    "Topic 13: science scientific scientists research people public world climate new like\n",
    "Topic 14: mars mission life surface martian planet water nasa said scientists\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Three new topics are introduced here, relative to the `TFIDF+NMF` or CorEx approaches. Some existing ones are modified in their appearance.\n",
    "2. One of these news topic names (`Dark Matter Theories`) is a weaker choice, based on the components words, and could be similar to the `Gravity and Black Holes` topic previously identified. Two topics could possibly be combined - `Brian Research` and `Learning and Memory` - resulting in 14 topics, as suggested by cross-validation with coherence-scores. NMF model hyper-parameter optimization could be explored to further study the topic selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "_ = plot_horiz_bar_gensim(\n",
    "    best_model,\n",
    "    id2word,\n",
    "    gensim_non_tfidf_mapping_dict[publication_name],\n",
    "    fig_size=(40, 35),\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll append the topic to the same row as each document in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "df_with_topics = format_topics_sentences(\n",
    "    best_model, corpus, df, gensim_non_tfidf_mapping_dict[publication_name]\n",
    ")\n",
    "display(df_with_topics.head(2))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-gensim-nmf-topics-combined-with-source-data\"></a>\n",
    "\n",
    "### 4.3. [Exploring Gensim NMF topics combined with source data](#exploring-gensim-nmf-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show a heatmap of the most popular topic by year, found by Gensim's implementation of NMF (recall this was done above without TFIDF Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df_with_topics.groupby([\"most_popular_topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"most_popular_topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "topics_by_timeframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"most_popular_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Most popular topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\n",
    "            \"title\": \"Most popular topic\",\n",
    "            \"field\": \"most_popular_topic\",\n",
    "            \"type\": \"nominal\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as main topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=Path().cwd() / \"reports\" / \"figures\" / \"my_heatmap.html\",\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of occurrences of the `\"Space Funding Bodies\"` as the most popular topic, relative to the year 1980\n",
    "- this will approximate the public interest in changes in this topic over the years investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = (\n",
    "    topics_by_timeframe[\n",
    "        topics_by_timeframe[\"most_popular_topic\"] == \"Space Funding Bodies\"\n",
    "    ]\n",
    "    .set_index(\"year\")[\"count\"]\n",
    "    .sort_index()\n",
    ")\n",
    "funds / funds.loc[funds.index.min()]\n",
    "funds = funds / funds.loc[funds.index.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "funds.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Cyclic variation in funding as main topic in article\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Funding\\n(rel. to 1981)\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The multi-modal aspects of the `TFIDF+NMF` and CorEx implementations showing a broadened peak centered at 2014, a weaker peak in articles published under this topic in 2004 and a peak in 2007also appear here. The choice of this topic was not as difficult here as it was for two of the other assigned topic names.\n",
    "2. Peak heights are distinctly weaker than those found in `TFIDF+NMF` and CorEx spproaches indicating documents previously assigned to this topic are now being placed in another topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "\n",
    "## 5. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Without `TFIDF` vectorization, `NMF` found three new topics that were not observed previously. The number of topics has some evidence of being one too many. If such an approach (`NMF` without `TFIDF` vectorization) is to be used further, then the performance of model hyperparameter optimization with 14 topics should be investigated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
