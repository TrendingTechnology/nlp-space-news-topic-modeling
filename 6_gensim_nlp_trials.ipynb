{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Gensim NLP trials](#gensim-nlp-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "\n",
    "import altair as alt\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, nmf\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import altair_datetime_heatmap, plot_horiz_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 26\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load joined data](#load-joined-data)\n",
    "3. [Create analysis pipeline](#create-analysis-pipeline)\n",
    "4. [Topic modeling with Gensim NMF](#topic-modeling-with-gensim-nmf)\n",
    "5. [Exploring Gensim NMF topics combined with source data](#exploring-gensim-nmf-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with NLP models on the joined news listings data in `data/processed/*_processed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "publication_name = \"guardian\"\n",
    "\n",
    "# Data locations\n",
    "data_dir_path = str(\n",
    "    Path().cwd() / \"data\" / \"processed\" / f\"{publication_name}_processed.csv\"\n",
    ")\n",
    "cloud_run = True\n",
    "\n",
    "# Custom stop words to include\n",
    "manual_stop_words = [\"nt\", \"ll\", \"ve\"]\n",
    "\n",
    "# Topic naming\n",
    "gensim_non_tfidf_mapping_dict = {\n",
    "    \"guardian\": {\n",
    "        0: \"Space Funding Bodies\",\n",
    "        1: \"Rocket Launches - Moon Landing and ISS\",\n",
    "        2: \"Discover of Sub-Atomic particles\",\n",
    "        3: \"Mars Exploration\",\n",
    "        4: \"Planetary Research\",\n",
    "        5: \"Shuttle Missions and Crashes\",\n",
    "        6: \"Academia\",\n",
    "        7: \"Gravity and Black Holes - Hawking\",\n",
    "        8: \"Black Body Radiation\",\n",
    "        9: \"Dark Matter theories\",\n",
    "        10: \"Pseudo space-science and Humanity - Opinion\",\n",
    "        11: \"Studying Comets and Meteors\",\n",
    "        12: \"Global Warming\",\n",
    "        13: \"Space Funding Bodies\",\n",
    "        14: \"Learning and Memory\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# General inputs\n",
    "limit = 20\n",
    "start = 10\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence(w2v_model, term_rankings):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations(term_rankings[topic_index], 2):\n",
    "            pair_scores.append(w2v_model.wv.similarity(pair[0], pair[1]))\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "\n",
    "def get_descriptor(all_terms, H, topic_index, top):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort(H[topic_index, :])[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append(all_terms[term_index])\n",
    "    return top_terms\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (\n",
    "            gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        )  # deacc=True removes punctuations\\\n",
    "\n",
    "\n",
    "def remove_stopwords(texts, manual_stop_words_list):\n",
    "    return [\n",
    "        [\n",
    "            word\n",
    "            for word in simple_preprocess(str(doc))\n",
    "            if word not in manual_stop_words_list\n",
    "        ]\n",
    "        for doc in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "class TokenGenerator:\n",
    "    def __init__(self, documents, stopwords):\n",
    "        self.documents = documents\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(\"Building Word2Vec model ...\")\n",
    "        for doc in self.documents:\n",
    "            tokens = []\n",
    "            for tok in self.tokenizer.findall(doc):\n",
    "                if tok in self.stopwords:\n",
    "                    tokens.append(\"<stopword>\")\n",
    "                elif len(tok) >= 2:\n",
    "                    tokens.append(tok)\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "def compute_coherence_values(corpus, id2word, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(\"Applying NMF for k=%d ...\" % num_topics)\n",
    "        model = nmf.Nmf(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=num_topics,\n",
    "            chunksize=len(corpus),\n",
    "            passes=10,\n",
    "            minimum_probability=0.01,\n",
    "            random_state=2,\n",
    "            normalize=True,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model, texts=texts, dictionary=id2word, coherence=\"c_v\"\n",
    "        )\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "def format_topics_sentences(model, corpus, df_source, topic_mapper_dict):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = [\n",
    "        \"most_popular_topic\",\n",
    "        \"Perc_Contribution\",\n",
    "        \"Topic_Keywords\",\n",
    "    ]\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    sent_topics_df = (\n",
    "        sent_topics_df.rename(columns={0: \"Text\"})\n",
    "        .astype({\"most_popular_topic\": int})\n",
    "        .replace(\"most_popular_topic\", topic_mapper_dict)\n",
    "    )\n",
    "    df_with_topics = sent_topics_df.merge(\n",
    "        df_source, left_index=True, right_index=True, how=\"inner\"\n",
    "    )\n",
    "    return df_with_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words from all packages\n",
    "# NLTK\n",
    "if not ((Path.cwd().parents[1]) / \"nltk_data\").exists():\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "# Spacy and sklearn\n",
    "spacy_stop_words = STOP_WORDS\n",
    "sklearn_stop_words = stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Assemble manual list of stop words\n",
    "spacy_not_in_sklearn = set(spacy_stop_words) - set(sklearn_stop_words)\n",
    "nltk_not_in_sklearn = set(nltk_stop_words) - set(sklearn_stop_words)\n",
    "all_stop_words = set(\n",
    "    list(set(sklearn_stop_words))\n",
    "    + list(spacy_not_in_sklearn)\n",
    "    + list(nltk_not_in_sklearn)\n",
    ")\n",
    "\n",
    "# Manually add to stop words\n",
    "for manual_stop_word in manual_stop_words:\n",
    "    all_stop_words.add(manual_stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-joined-data\"></a>\n",
    "\n",
    "## 2. [Load joined data](#load-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the joined data from from a publication, stored at `data/processed/<publication-name>_processed.csv`, into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(data_dir_path))\n",
    "df = df[[\"text\", \"year\"]]\n",
    "print(df.shape[0])\n",
    "display(df.head())\n",
    "# print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(data_dir_path))\n",
    "df = df[[\"text\", \"year\"]]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = df.loc[:, \"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words=all_stop_words,\n",
    "    min_df=20,\n",
    "    max_features=None,\n",
    "    binary=False,\n",
    "    strip_accents=None,\n",
    ")\n",
    "\n",
    "docs_terms = vectorizer.fit_transform(corpus_raw)\n",
    "print(\n",
    "    f\"Created {docs_terms.shape[0]:0d} X {docs_terms.shape[1]:0d} TF-IDF-normalized document-term matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "topic_models = []\n",
    "for num_topics in range(start, limit + 1):\n",
    "    print(f\"Applying NMF with {num_topics:0d} topics...\")\n",
    "    model = NMF(n_components=num_topics, max_iter=200)\n",
    "    model_transformed = model.fit_transform(docs_terms)\n",
    "    factors_dict = model.components_\n",
    "    topic_models.append((num_topics, model_transformed, factors_dict))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "docgen = TokenGenerator(corpus_raw, all_stop_words)\n",
    "w2v_model = gensim.models.Word2Vec(docgen, size=500, min_count=20, sg=1)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model has %d terms\" % len(w2v_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "k_values = []\n",
    "coherences = []\n",
    "for (k, W, H) in topic_models:\n",
    "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
    "    term_rankings = []\n",
    "    for topic_index in range(k):\n",
    "        term_rankings.append(get_descriptor(terms, H, topic_index, 10))\n",
    "    # Now calculate the coherence based on our Word2vec model\n",
    "    k_values.append(k)\n",
    "    coherences.append(calculate_coherence(w2v_model, term_rankings))\n",
    "    print(\"K=%02d: Coherence=%.4f\" % (k, coherences[-1]))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "# create the line plot\n",
    "ax = plt.plot(k_values, coherences)\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Mean Coherence\")\n",
    "# add the points\n",
    "plt.scatter(k_values, coherences, s=120)\n",
    "# find and annotate the maximum point on the plot\n",
    "ymax = max(coherences)\n",
    "xpos = coherences.index(ymax)\n",
    "best_k = k_values[xpos]\n",
    "plt.annotate(\n",
    "    \"k=%d\" % best_k,\n",
    "    xy=(best_k, ymax),\n",
    "    xytext=(best_k, ymax),\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "# get the model that we generated earlier.\n",
    "W = topic_models[k - start][1]\n",
    "H = topic_models[k - start][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_index in range(k):\n",
    "    descriptor = get_descriptor(terms, H, topic_index, 10)\n",
    "    str_descriptor = \", \".join(descriptor)\n",
    "    print(\"Topic %02d: %s\" % (topic_index + 1, str_descriptor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-modeling-with-gensim-nmf\"></a>\n",
    "\n",
    "## 4. [Topic modeling with Gensim NMF](#topic-modeling-with-gensim-nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the NLP analysis pipeline to retrieve topics from the corpus using Gensim's implementation of NMF. This will be done without TFIDF Vectorization from either [`sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn-feature-extraction-text-tfidfvectorizer) or [`gensim`](https://radimrehurek.com/gensim/models/tfidfmodel.html#gensim.models.tfidfmodel.TfidfModel) itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "data_words = list(sent_to_words(corpus_raw))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(\n",
    "    data_words, min_count=5, threshold=100\n",
    ")  # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words, all_stop_words)\n",
    "\n",
    "# Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_nostops, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "# Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word = corpora.Dictionary(data_words_nostops)\n",
    "\n",
    "# Term Document Frequency for corpus\n",
    "# corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "corpus = [id2word.doc2bow(text) for text in data_words_nostops]\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    texts=data_words_nostops,\n",
    "    # texts=data_lemmatized,\n",
    "    limit=limit,\n",
    "    start=start,\n",
    "    step=step,\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "df_with_topics = format_topics_sentences(\n",
    "    best_model, corpus, df, gensim_non_tfidf_mapping_dict[publication_name]\n",
    ")\n",
    "display(df_with_topics.head(2))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-gensim-nmf-topics-combined-with-source-data\"></a>\n",
    "\n",
    "## 5. [Exploring Gensim NMF topics combined with source data](#exploring-gensim-nmf-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show a heatmap of the most popular topic by year, found by Gensim's implementation of NMF (recall this was done above without TFIDF Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df_with_topics.groupby([\"most_popular_topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"most_popular_topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "topics_by_timeframe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"most_popular_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Most popular topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\"title\": \"Most popular topic\", \"field\": \"most_popular_topic\", \"type\": \"nominal\",},\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as main topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=Path().cwd() / \"reports\" / \"figures\" / \"my_heatmap.html\",\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of occurrences of the `\"Space Funding Bodies\"` as the most popular topic, relative to the year 1980\n",
    "- this will approximate the public interest in changes in this topic over the years investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = (\n",
    "    topics_by_timeframe[\n",
    "        # topics_by_timeframe[\"most_popular_topic\"] == \"Space Funding Bodies\"\n",
    "        topics_by_timeframe[\"most_popular_topic\"]\n",
    "        == 0\n",
    "    ]\n",
    "    .set_index(\"year\")[\"count\"]\n",
    "    .sort_index()\n",
    ")\n",
    "funds / funds.loc[funds.index.min()]\n",
    "funds = funds / funds.loc[funds.index.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "funds.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Cyclic variation in funding as main topic in article\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Funding\\n(rel. to 1981)\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Both the `sklearn` NMF and CorEx implementations show a broadened peak centered at 2014 and a weaker+narrower peak in articles published under this topic in 2004. The latter appears here, and is the strongest peak, while the former (2014 peak) is not evident."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
