{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CorEx trials](#corex-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.sparse as ss\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import altair_datetime_heatmap, plot_horiz_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 26\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load joined data](#load-joined-data)\n",
    "3. [Create analysis model](#create-analysis-model)\n",
    "4. [Topic modeling](#topic-modeling)\n",
    "5. [Exploring topics combined with source data](#exploring-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with NLP models using CorEx on the joined news listings data in `data/processed/*_processed.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.path.abspath(os.getcwd())\n",
    "processed_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "publication_name = \"guardian\"\n",
    "cloud_run = False\n",
    "\n",
    "# Data locations\n",
    "data_dir_path = os.path.join(processed_data_dir, f\"{publication_name}_processed.csv\")\n",
    "\n",
    "# Topic naming\n",
    "mapping_dict = {\n",
    "    \"nytimes\": {\n",
    "        \"component_1\": \"Academia\",\n",
    "        \"component_2\": \"Shuttle Missions (no Crashes)\",\n",
    "        \"component_3\": \"Digital\",\n",
    "        \"component_4\": \"Mars Exploration\",\n",
    "        \"component_5\": \"Imaging Stars - Astronomy\",\n",
    "        \"component_6\": \"Rocket Launches - Testing and Moon Landing\",\n",
    "        \"component_7\": \"Dark Matter theories\",\n",
    "        \"component_8\": \"Planetary Research\",\n",
    "        \"component_9\": \"Space Funding Bodies\",\n",
    "        \"component_10\": \"ISS - USA and Russian segments\",\n",
    "        \"component_11\": \"Gravity and Black Holes - Hawking\",\n",
    "        \"component_12\": \"Global Warming\",\n",
    "        \"component_13\": \"Studying Comets and Meteors (by children)\",\n",
    "        \"component_14\": \"Soviet Union Spy Satellites\",\n",
    "        \"component_15\": \"Discover of Sub-Atomic particles\",\n",
    "    },\n",
    "    \"guardian\": {\n",
    "        \"component_1\": \"Academia\",\n",
    "        \"component_2\": \"ISS - USA and Russian segments\",\n",
    "        \"component_3\": \"Mars Exploration\",\n",
    "        \"component_4\": \"Imaging Stars - Astronomy\",\n",
    "        \"component_5\": \"Studying Comets and Meteors\",\n",
    "        \"component_6\": \"Discover of Sub-Atomic particles\",\n",
    "        \"component_7\": \"Rocket Launches - Moon Landing\",\n",
    "        \"component_8\": \"Shuttle Missions and Crashes\",\n",
    "        \"component_9\": \"Saturn Research\",\n",
    "        \"component_10\": \"Space Funding Bodies\",\n",
    "        \"component_11\": \"Objects crashing into Earth\",\n",
    "        \"component_12\": \"Gravity and Black Holes - Hawking\",\n",
    "        \"component_13\": \"Rocket Launches - Testing\",\n",
    "        \"component_14\": \"Pluto Research\",\n",
    "        \"component_15\": \"Global Warming\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# CorEx inputs\n",
    "corex_anchors = {\n",
    "    \"nytimes\": [\n",
    "        [\"research\", \"science\", \"university\"],\n",
    "        [\"space\", \"shuttle\", \"mission\", \"launch\", \"astronaut\"],\n",
    "        [\"computer\", \"disk\", \"software\", \"memory\"],\n",
    "        [\"mars\", \"rover\", \"life\"],\n",
    "        [\"stars\", \"galaxy\", \"telescope\"],\n",
    "        [\"moon\", \"lunar\", \"rocket\", \"nasa\", \"spacex\"],\n",
    "        [\"universe\", \"theory\", \"matter\"],\n",
    "        [\"planet\", \"solar\", \"spacecraft\", \"asteroid\"],\n",
    "        [\"science\", \"research\", \"budget\", \"education\"],\n",
    "        [\"station\", \"space\", \"mir\", \"nasa\"],\n",
    "        [\"black\", \"hole\", \"hawking\", \"gravity\"],\n",
    "        [\"warming\", \"climate\", \"ice\", \"carbon\"],\n",
    "        [\"comet\", \"meteor\", \"halley\"],\n",
    "        [\"soviet\", \"satellite\", \"weapons\"],\n",
    "        [\"particles\", \"quantum\", \"neutrino\", \"theory\"],\n",
    "    ],\n",
    "    \"guardian\": [\n",
    "        [\"people\", \"science\", \"brain\"],\n",
    "        [\"station\", \"space\", \"mir\", \"nasa\"],\n",
    "        [\"mars\", \"rover\", \"life\"],\n",
    "        [\"stars\", \"galaxy\", \"telescope\", \"astronomer\"],\n",
    "        [\"comet\", \"meteor\", \"lander\", \"dust\"],\n",
    "        [\"particles\", \"higgs\", \"collider\", \"matter\"],\n",
    "        [\"moon\", \"lunar\", \"rocket\", \"nasa\", \"apollo\"],\n",
    "        [\"space\", \"shuttle\", \"mission\", \"launch\", \"crash\", \"astronaut\"],\n",
    "        [\"cassini\", \"titan\", \"saturn\"],\n",
    "        [\"science\", \"research\", \"budget\", \"education\"],\n",
    "        [\"rock\", \"collision\", \"earth\", \"asteroid\", \"impact\"],\n",
    "        [\"black\", \"hole\", \"universe\", \"gravity\"],\n",
    "        [\"space\", \"launch\", \"rocket\", \"nasa\", \"spacex\"],\n",
    "        [\"pluto\", \"horizons\", \"dwarf\"],\n",
    "        [\"warming\", \"climate\", \"ice\", \"carbon\"],\n",
    "    ],\n",
    "}\n",
    "corex_anchor_strength = 4\n",
    "\n",
    "# General inputs\n",
    "n_topics_wanted = len(corex_anchors[publication_name])\n",
    "number_of_words_per_topic_to_show = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-joined-data\"></a>\n",
    "\n",
    "## 2. [Load joined data](#load-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the joined data from from a publication, stored at `data/processed/<publication-name>_processed.csv`, into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir_path)\n",
    "df = df[[\"text\", \"year\"]]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.loc[:, \"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-analysis-model\"></a>\n",
    "\n",
    "## 3. [Create analysis model](#create-analysis-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vectorizer, we will instantiate 1 option for tokenization\n",
    "- `vectorizer`\n",
    "  - this will use the string tokenization built in to [`scikit-learn`'s `CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    stop_words=\"english\",  # \"all_stop_words\" or \"english\"\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=20000,\n",
    "    binary=True,\n",
    "    strip_accents=\"ascii\",\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-modeling\"></a>\n",
    "\n",
    "## 4. [Topic modeling](#topic-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the NLP analysis model to retrieve topics from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = vectorizer.fit_transform(corpus)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = ct.Corex(\n",
    "    n_hidden=n_topics_wanted, words=words, max_iter=200, verbose=False, seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "topic_model.fit(\n",
    "    doc_word,\n",
    "    words=words,\n",
    "    docs=corpus,\n",
    "    anchors=corex_anchors[publication_name],\n",
    "    anchor_strength=corex_anchor_strength,\n",
    ")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll print all topics found from the CorEx topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# model = model = NMF(n_components=n_topics_wanted, init=\"random\", random_state=0)\n",
    "# doc_word = vectorizer.fit_transform(corpus)\n",
    "# doc_topic = model.fit_transform(doc_word)\n",
    "# topic_word = pd.DataFrame(\n",
    "#     model.components_.round(3),\n",
    "#     index=[f\"component_{k+1}\" for k in range(n_topics_wanted)],\n",
    "#     columns=vectorizer.get_feature_names(),\n",
    "# )\n",
    "# topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row_list = []\n",
    "for topic_num in range(1, n_topics_wanted + 1):\n",
    "    df_row_topic_words = (\n",
    "        pd.DataFrame.from_records(\n",
    "            topic_model.get_topics(n_words=10, topic=topic_num - 1),\n",
    "            columns=[\"Word\", f\"component_{topic_num}\"],\n",
    "        )\n",
    "        .round(3)\n",
    "        .set_index(\"Word\")\n",
    "        .T\n",
    "    )\n",
    "    df_row_list.append(df_row_topic_words)\n",
    "topic_word = pd.concat(df_row_list, axis=0, sort=False).fillna(0)\n",
    "# topic_word.index = topic_word.index.to_series().map(mapping_dict[publication_name])\n",
    "# display(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_horiz_bar(\n",
    "    topic_word.T,\n",
    "    ptitle=[f\"Topic {k+1}\" for k in range(n_topics_wanted)],\n",
    "    y_tick_mapper_list=list(mapping_dict[publication_name].values()),\n",
    "    fig_size=(37, 6),\n",
    "    xspacer=0.001,\n",
    "    yspacer=0.3,\n",
    "    ytick_font_size=18,\n",
    "    title_font_size=20,\n",
    "    annot_font_size=16,\n",
    "    n_bars=number_of_words_per_topic_to_show,\n",
    "    n_plots=topic_word.T.shape[1],\n",
    "    n_cols=n_topics_wanted,\n",
    "    show_bar_labels=False,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The `Saturn Research` topic name appears a weaker assignment here for the Guardian than using NMF in `4_nlp_trials.ipynb`. The other topics appear reasonably well distinguished from eachother in the widths of the bars in the chart and from the words (bar labels) themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "if not cloud_run:\n",
    "    fig.savefig(\n",
    "        os.path.join(\n",
    "            PROJ_ROOT_DIR,\n",
    "            \"reports\",\n",
    "            \"figures\",\n",
    "            f\"{publication_name}_nmf_topics_corex.png\",\n",
    "        ),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=300,\n",
    "    )\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topics(n_words=10)\n",
    "topic_words_dict = {}\n",
    "for n, topic in enumerate(topics):\n",
    "    topic_words, _ = zip(*topic)\n",
    "    topic_words_dict[n] = topic_words\n",
    "    print(f\"{n+1}: {', '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame.from_dict(topic_words_dict, orient=\"index\")\n",
    "df_topics.index = [f\"topic {i+1}\" for i in range(n_topics_wanted)]\n",
    "df_topics.columns = [f\"word {i+1}\" for i in range(df_topics.shape[1])]\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(\n",
    "    topic_model.predict(doc_word),\n",
    "    columns=[\n",
    "        # f\"topic {i+1}: {', '.join(all_topic_words)}\"\n",
    "        f\"component_{i+1}\"\n",
    "        for i, all_topic_words in topic_words_dict.items()\n",
    "    ],\n",
    ")\n",
    "predictions = predictions.rename(columns=mapping_dict[publication_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df[[\"text\", \"year\"]].merge(\n",
    "    predictions, how=\"inner\", left_index=True, right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all topics per document\n",
    "topics_per_row = df_predictions[df_predictions == True].stack().index.tolist()\n",
    "df_topics_per_row = pd.DataFrame(topics_per_row).set_index(0)\n",
    "df_topics_per_row.columns = [\"topic\"]\n",
    "df_predictions = df_predictions.merge(\n",
    "    df_topics_per_row, how=\"inner\", left_index=True, right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions[\"topic\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-topics-combined-with-source-data\"></a>\n",
    "\n",
    "## 5. [Exploring topics combined with source data](#exploring-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show a heatmap of the most popular topic by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df_predictions.groupby([\"topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "topics_by_timeframe.rename(columns={\"topic\": \"assigned_topic\"}, inplace=True)\n",
    "topics_by_timeframe.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"assigned_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Assigned topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\"title\": \"Assigned topic\", \"field\": \"assigned_topic\", \"type\": \"nominal\",},\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as assigned topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=os.path.join(PROJ_ROOT_DIR, \"reports\", \"figures\", \"my_heatmap.html\"),\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of occurrences of the `\"Space Funding Bodies\"` as the most popular topic, relative to the year 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = (\n",
    "    topics_by_timeframe[topics_by_timeframe[\"assigned_topic\"] == \"Space Funding Bodies\"]\n",
    "    .set_index(\"year\")[\"count\"]\n",
    "    .sort_index()\n",
    ")\n",
    "funds / funds.loc[funds.index.min()]\n",
    "funds = funds / funds.loc[funds.index.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "funds.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Cyclic variation in funding as one of the assigned topics in article\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Funding\\n(rel. to 1981)\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of words in the `\"text\"` of the article\n",
    "- this will approximate the length of articles over the years investigated\n",
    "- this will approximate the public interest in changes in this topic over the years investigated\n",
    "  - if public interest decreases with time, it could be assumed to result in shorter articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_chars\"] = df[\"text\"].str.split().str.len()\n",
    "word_count = df[[\"article_chars\", \"year\"]].groupby([\"year\"])[\"article_chars\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "word_count.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Words in article\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Word count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "df[\"year\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8\n",
    ")\n",
    "ax.set_title(\n",
    "    \"Articles per year\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Article count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "pd.DataFrame(word_count).merge(\n",
    "    pd.DataFrame(df[\"year\"].value_counts()),\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").plot.scatter(x=\"year\", y=\"article_chars\", ax=ax)\n",
    "ax.set_title(\n",
    "    \"Word count increases with number of articles\", fontsize=18, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"Articles per year\", fontweight=\"bold\")\n",
    "h = plt.ylabel(\"Word count\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the\n",
    "- New York Times\n",
    "  - between 1981 and 2004, article length was fairly consistent\n",
    "  - there was a significant drop in the years 2005 and 2012\n",
    "  - since 2013, article length has been steadily increasing.\n",
    "- Guardian\n",
    "  - prior to 1999, very few articles were published and article length (from the published ones) was fairly consistently low\n",
    "  - there was a drop from the years 2006 to 2010, likely coinciding to the [Great Recession](https://www.investopedia.com/terms/g/great-recession.asp)\n",
    "  - since 2010, article length increased until 2015 and has been dropping since then"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
