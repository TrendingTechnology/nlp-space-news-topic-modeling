{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scrape article URLs](#scrape-article-urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.url_extraction_helpers\n",
    "from src.url_extraction_helpers import (\n",
    "    generate_space_archive_url,\n",
    "    generate_nytimes_api_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Retrieve Space.com metadata from archive](#retrieve-space.com-metadata-from-archive)\n",
    "3. [Retrieve Guardian newspaper metadata from API](#retrieve-guardian-newpaper-metadata-from-api)\n",
    "4. [Retrieve Hubble telescope metadata from archive](#retrieve-hubble-telescope-metadata-from-archive)\n",
    "5. [Retrieve New York Times newspaper metadata from API](#retrieve-new-york-times-newpaper-metadata-from-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use archives or APIs to retrieve URLs for articles of various publications and export data to `*.csv` files in `data/raw/*_urls.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Guardian\n",
    "guardian_from_date = \"1950-01-01\"\n",
    "guardian_to_date = \"2019-11-01\"\n",
    "guardian_section = \"science\"\n",
    "guardian_query = \"space\"\n",
    "guardian_start_page_num = 1\n",
    "guardian_num_pages_wanted = 49\n",
    "guardian_api = os.get_env(\"GUARDIAN_API_KEY\")\n",
    "guardian_query_min_delay = 2\n",
    "guardian_query_max_delay = 4\n",
    "\n",
    "# Hubble\n",
    "hubble_article_fields_available = [\"name\", \"news_id\", \"url\"]\n",
    "\n",
    "# Space.com\n",
    "space_com_years = list(range(1999, 2019 + 1))\n",
    "\n",
    "# NY Times\n",
    "nytimes_api = os.get_env(\"NYTIMES_API_KEY\")\n",
    "nytimes_query = \"space\"\n",
    "nytimes_begin_date = \"19500101\"  # \"19500101\"\n",
    "nytimes_end_date = \"20191101\"  # \"20191101\"\n",
    "nytimes_start_page_num = 0\n",
    "nytimes_num_pages_wanted = -1\n",
    "nytimes_newspaper_lang = \"en\"\n",
    "\n",
    "# Other inputs\n",
    "data_dir = str(Path().cwd() / \"data\" / \"raw\")\n",
    "list_of_urls_file = {\n",
    "    \"space\": Path(data_dir) / \"space_com_urls.csv\",\n",
    "    \"guardian\": Path(data_dir) / \"guardian_urls.csv\",\n",
    "    \"hubble\": Path(data_dir) / \"hubble_urls.csv\",\n",
    "    \"nytimes\": Path(data_dir) / \"nytimes_urls.csv\",\n",
    "}\n",
    "urls = {\n",
    "    \"guardian\": \"https://content.guardianapis.com/search\",\n",
    "    \"hubble\": \"http://hubblesite.org/api/v3/news?page=all\",\n",
    "    \"space\": \"https://www.space.com/archive\",\n",
    "}\n",
    "\n",
    "# API Query inputs\n",
    "query_params = {\n",
    "    \"guardian\": {\n",
    "        \"section\": guardian_section,\n",
    "        \"from-date\": guardian_from_date,\n",
    "        \"to-date\": guardian_to_date,\n",
    "        \"order-by\": \"oldest\",\n",
    "        \"page-size\": 100,\n",
    "        \"q\": guardian_query,\n",
    "        \"api-key\": guardian_api,\n",
    "    },\n",
    "    \"hubble\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardian_url = (\n",
    "#     \"https://content.guardianapis.com/search?\"\n",
    "#     f\"section={section}&\"\n",
    "#     f\"from-date={from_date}&\"\n",
    "#     f\"to-date={to_date}&\"\n",
    "#     \"order-by=oldest&\"\n",
    "#     \"page={0}&\"\n",
    "#     \"page-size=100&\"\n",
    "#     f\"q='{query}'&\"\n",
    "#     f\"api-key={api}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-space.com-metadata-from-archive\"></a>\n",
    "\n",
    "## 2. [Retrieve Space.com metadata from archive](#retrieve-space.com-metadata-from-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will retrieve the URL for articles and other metadata from space.com by doing the following for each year in which to retrieve data\n",
    "1. Programmatically assemble URL to archive\n",
    "2. Scrape web link for articles and store in dictionary\n",
    "3. Convert dictionary of urls and metadata into `DataFrame`\n",
    "4. Append archive URL to `DataFrame`\n",
    "5. Concatenate `DataFrame`s of metadata into single `DataFrame`\n",
    "6. Export `DataFrame` of metadata to `*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space.com urls to file\n",
    "dfs_space_urls = []\n",
    "# Loop over all years to be queried, assemble archive url and retrieve article details\n",
    "for year in space_com_years:\n",
    "    dfs_space_urls_per_year = []\n",
    "    # 1. Assemble archive url\n",
    "    space_com_archive_urls = generate_space_archive_url(year, urls[\"space\"])\n",
    "    # Loop over all archive urls to be queried and retrieve article details\n",
    "    for space_com_archive_url in space_com_archive_urls:\n",
    "        print(f\"Retrieving all article URLs from space.com archive at: {sp_url}\")\n",
    "        d_url = {}\n",
    "        page_response = requests.get(space_com_archive_url, timeout=5)\n",
    "        soup = BeautifulSoup(page_response.content, \"lxml\")\n",
    "        # print(soup.prettify())\n",
    "        # dates_li = soup.findAll(\"li\", {\"class\": \"list-title date-heading\"})\n",
    "        # all_dates = [str(date.text)[:-2] + f\", {str(year)}\" for date in dates_li]\n",
    "        # 2. Scrape article url and store in dict\n",
    "        li_all = soup.findAll(\"li\", {\"class\": \"day-article\"})\n",
    "        d_url[year] = [li.find(\"a\")[\"href\"] for li in li_all]\n",
    "        # 3. Convert dict of urls to DataFrame of urls\n",
    "        df_space_urls_all_months = pd.DataFrame.from_dict(d_url, orient=\"index\").T\n",
    "        # 4. Append archive url to DataFrame\n",
    "        df_space_urls_all_months[\"archive_url\"] = space_com_archive_url\n",
    "        df_space_urls_all_months.rename(columns={year: \"url\"}, inplace=True)\n",
    "        dfs_space_urls_per_year.append(df_space_urls_all_months)\n",
    "    # 5. Concatenate DataFrames of yearly (single-year) urls\n",
    "    df_space_urls = pd.concat(dfs_space_urls_per_year, axis=0, ignore_index=True).drop_duplicates()\n",
    "    dfs_space_urls.append(df_space_urls)\n",
    "# 6. Concatenate DataFrames across all years\n",
    "df_all_space_urls = pd.concat(dfs_space_urls, axis=0, ignore_index=True).drop_duplicates()\n",
    "display(df_all_space_urls)\n",
    "# 7. Export DataFrame of metadata to *.csv file\n",
    "df_all_space_urls.to_csv(list_of_urls_file[\"space\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-guardian-newpaper-metadata-from-api\"></a>\n",
    "\n",
    "## 3. [Retrieve Guardian newspaper metadata from API](#retrieve-guardian-newpaper-metadata-from-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will retrieve the URL for articles and other metadata from the Guardian newspaper [API](https://open-platform.theguardian.com/documentation/) ([data explorer](https://open-platform.theguardian.com/explore/)) by doing the following for each year in which to retrieve data\n",
    "1. Find maximum number of pages of results available for query\n",
    "2. Set the maximum page number to be queried\n",
    "3. Retrieve query response per page\n",
    "4. Convert jsonified response into dictionary\n",
    "5. Extract various metadata, including article URL, of converted response and store in separate dictionary\n",
    "6. Convert dictionary of urls and metadata into `DataFrame`\n",
    "7. Append page number of DataFrame\n",
    "8. Concatenate `DataFrame`s of metadata into single `DataFrame`\n",
    "9. Filter `DataFrame` to retain articles and remove blogs\n",
    "10. Export `DataFrame` of metadata to `*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardian urls to file\n",
    "dfs_guardian_details = []\n",
    "# 1. Find maximum number of pages of results available\n",
    "guardian_max_pages_returned = requests.get(\n",
    "    urls[\"guardian\"], params=query_params[\"guardian\"]\n",
    ").json()[\"response\"][\"pages\"]\n",
    "# 2. Set the maximum page number to be queried\n",
    "if guardian_num_pages_wanted == -1:\n",
    "    guardian_max_page_num = guardian_max_pages_returned\n",
    "    guardian_pages_to_use = \"all available\"\n",
    "else:\n",
    "    guardian_max_page_num = guardian_start_page_num + guardian_num_pages_wanted\n",
    "    guardian_pages_to_use = \"requested\"\n",
    "print(\n",
    "    f\"Retrieving articles from {guardian_pages_to_use} pages, \"\n",
    "    f\"number of requested pages ({guardian_num_pages_wanted}), \"\n",
    "    f\"number of available pages ({guardian_max_page_num})\"\n",
    ")\n",
    "# Loop over all pages to be queried and retrieve article details\n",
    "for page in range(guardian_start_page_num, guardian_max_page_num):\n",
    "    d = {}\n",
    "    query_params[\"guardian\"][\"page\"] = page\n",
    "    # 3. Send GET request to API and retrieve response\n",
    "    r = requests.get(urls[\"guardian\"], params=query_params[\"guardian\"])\n",
    "    # print(r.json().keys())\n",
    "    # 4. Get results dict from response attribute of jsonified response\n",
    "    rdocs = r.json()[\"response\"][\"results\"]\n",
    "    print(f\"Page: {page}, Found: {len(rdocs)} articles\")\n",
    "    # 5. Extract various attributes (metadata) of response json and store in dict\n",
    "    for key in [\n",
    "        \"webUrl\",\n",
    "        \"id\",\n",
    "        \"webPublicationDate\",\n",
    "        \"apiUrl\",\n",
    "        \"webTitle\",\n",
    "        \"document_type\",\n",
    "        \"sectionId\",\n",
    "        \"sectionName\",\n",
    "        \"type\",\n",
    "        \"isHosted\",\n",
    "        \"pillarId\",\n",
    "        \"pillarName\",\n",
    "    ]:\n",
    "        d[key] = []\n",
    "        for rr in rdocs:\n",
    "            try:\n",
    "                rr[key]\n",
    "                d[key].append(rr[key])\n",
    "            except Exception as e:\n",
    "                d[key].append(None)\n",
    "    print(f\"Retrieved {len(rdocs)} article details from page {page}\")\n",
    "    # 6. Convert dict of urls to DataFrame of urls\n",
    "    df_guardian_article = pd.DataFrame.from_dict(d, orient=\"index\").T\n",
    "    # 7. Append page number of DataFrame\n",
    "    df_guardian_article[\"page\"] = page\n",
    "    dfs_guardian_details.append(df_guardian_article)\n",
    "    # Pause between pages\n",
    "    if page != (guardian_start_page_num + guardian_num_pages_wanted) - 1:\n",
    "        random_sleep_time = randint(guardian_query_min_delay, guardian_query_max_delay)\n",
    "        print(f\"Pausing for {random_sleep_time} seconds before retrieving from page {page+1}\\n\")\n",
    "        sleep(random_sleep_time)\n",
    "# 8. Concatenate DataFrames across all pages\n",
    "df_guardian_details = pd.concat(dfs_guardian_details, axis=0, ignore_index=True).drop_duplicates()\n",
    "# 9. Filter DataFrame to retain articles and remove blogs\n",
    "df_guardian_details = df_guardian_details.loc[\n",
    "    (df_guardian_details[\"type\"] == \"article\") &\n",
    "    (~df_guardian_details[\"webUrl\"].str.contains(\"blog\"))\n",
    "]\n",
    "print(df_guardian_details.shape[0])\n",
    "display(df_guardian_details)\n",
    "# 10. Export DataFrame of metadata to *.csv file\n",
    "df_guardian_details.to_csv(list_of_urls_file[\"guardian\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-hubble-telescope-metadata-from-archive\"></a>\n",
    "\n",
    "## 4. [Retrieve Hubble telescope metadata from archive](#retrieve-hubble-telescope-metadata-from-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will retrieve the URL for articles and other metadata from the Hubble telescope news release [API](http://hubblesite.org/api/documentation#news) by doing the following for the only page (covering all years) in which to retrieve data\n",
    "1. Retrieve query response per page\n",
    "2. Convert jsonified response into dictionary\n",
    "3. Extract various metadata, one at a time, including article URL, of converted response and store in separate dictionary\n",
    "4. Convert dictionary of urls and metadata into `DataFrame`\n",
    "5. Concatenate `DataFrame`s of metadata into single `DataFrame`\n",
    "6. Rename `DataFrame` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubble urls to file - 1/2\n",
    "dfs_hubble_article_details = []\n",
    "# 1. Send GET request to API and retrieve response\n",
    "r = requests.get(urls[\"hubble\"], params=query_params[\"hubble\"])\n",
    "# 2. Get results dict from jsonified response\n",
    "rdocs = r.json()\n",
    "# 3. Extract various attributes of response json and store in dict\n",
    "for key in hubble_article_fields_available:\n",
    "    d = {}\n",
    "    d[key] = []\n",
    "    for rr in rdocs:\n",
    "        try:\n",
    "            rr[key]\n",
    "            d[key].append(rr[key])\n",
    "        except Exception as e:\n",
    "            d[key].append(None)\n",
    "        print(f\"Retrieved article details for news_id: {rr['news_id']}, from {rr['url']}\")\n",
    "    # 4. Convert dict of urls to DataFrame of urls\n",
    "    df_hubble_article_details = pd.DataFrame.from_dict(d, orient=\"index\").T\n",
    "    dfs_hubble_article_details.append(df_hubble_article_details)\n",
    "# 5. Concatenate DataFrames (horizontally) across all attributes\n",
    "dfs_hubble_articles_details = pd.concat(\n",
    "    dfs_hubble_article_details, axis=1, ignore_index=True\n",
    ").drop_duplicates()\n",
    "# 6. Rename columns\n",
    "dfs_hubble_articles_details.columns = hubble_article_fields_available\n",
    "print(dfs_hubble_articles_details.shape[0])\n",
    "assert dfs_hubble_articles_details.shape[0] == len(rdocs)\n",
    "dfs_hubble_articles_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the details of each news release from a separate API endpoint. To do this, we will do the following for each news article listed `id`\n",
    "1. Retrieve query response per `id`\n",
    "2. Convert jsonified response into dictionary\n",
    "3. Extract mission, publication and abstract attributes of response json and store in new columns of above `DataFrame`\n",
    "4. Export `DataFrame` of metadata to `*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hubble urls to file - 2/2\n",
    "# Append mission and publication columns\n",
    "dfs_hubble_articles_details[\"mission\"] = np.nan\n",
    "dfs_hubble_articles_details[\"publication\"] = np.nan\n",
    "# Populate mission and publication columns\n",
    "for index, row in dfs_hubble_articles_details.iterrows():\n",
    "    # print(row['news_id'])\n",
    "    # 1. Send GET request to news_release API and retrieve resoonse\n",
    "    r_news_release = requests.get(\"http://hubblesite.org/api/v3/news_release/\" + row[\"news_id\"])\n",
    "    # 2. Get results dict from jsonified response\n",
    "    rdocs_news_release = r_news_release.json()\n",
    "    # print(rdocs_news_release)\n",
    "    # 3. Extract various attributes of response json and store in dict and populate newly added mission\n",
    "    # and publication columns\n",
    "    for key in [\n",
    "        \"mission\",\n",
    "        \"publication\",\n",
    "        \"abstract\",\n",
    "    ]:\n",
    "        try:\n",
    "            rdocs_news_release[key]\n",
    "            dfs_hubble_articles_details.loc[index, key] = rdocs_news_release[key]\n",
    "        except Exception as e:\n",
    "            dfs_hubble_articles_details.loc[index, key] = None\n",
    "    print(\n",
    "        f\"Retrieved article {index} details for publication: {dfs_hubble_articles_details.loc[index, 'publication']}, \"\n",
    "        f\"for mission {dfs_hubble_articles_details.loc[index, 'mission']}\"\n",
    "    )\n",
    "print(dfs_hubble_articles_details.shape[0])\n",
    "display(dfs_hubble_articles_details)\n",
    "# 4. Export DataFrame of all metadata (from both API endpoints) to *.csv\n",
    "dfs_hubble_articles_details.to_csv(list_of_urls_file[\"hubble\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"retrieve-new-york-times-newpaper-metadata-from-api\"></a>\n",
    "\n",
    "## 5. [Retrieve New York Times newspaper metadata from API](#retrieve-new-york-times-newpaper-metadata-from-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When querying the NY Times API, there are 2 important considerations to take into account\n",
    "- for how to navigate through pages, see the [API documentation](https://developer.nytimes.com/docs/articlesearch-product/1/overview), under **Filtering Your Search** > **Pagination**\n",
    "  > The Article Search API returns a max of 10 results at a time. The meta node in the response contains the total number of matches (\"hits\") and the current offset. Use the page query parameter to paginate thru results (page=0 for results 1-10, page=1 for 11-20, ...). You can paginate thru up to 100 pages (1,000 results). If you get too many results try filtering by date range.\n",
    "- for how to stay within the API's call limits, see the [FAQs](https://developer.nytimes.com/faq#a11), under **11. Is there an API call limit?**\n",
    "  > Yes, there are two rate limits per API: 4,000 requests per day and 10 requests per minute. You should sleep 6 seconds between calls to avoid hitting the per minute rate limit. If you need a higher rate limit, please contact us at code@nytimes.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, we will retrieve the URL for articles and other metadata from the New York Times newspaper [API](https://developer.nytimes.com/docs/articlesearch-product/1/overview) by doing the following for each year in which to retrieve data\n",
    "1. Find maximum number of pages of results available for query\n",
    "2. Set the maximum page number to be queried\n",
    "3. Retrieve query response per page\n",
    "4. Extract various metadata, including article URL, of converted response and store in separate dictionary\n",
    "5. Convert dictionary of urls and metadata into `DataFrame`\n",
    "6. Append page number of DataFrame\n",
    "7. Concatenate `DataFrame`s of metadata into single `DataFrame`\n",
    "8. Filter `DataFrame` to retain articles and remove blogs\n",
    "9. Export `DataFrame` of metadata to `*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NY Times urls to file\n",
    "dfs_nytimes_article_details = []\n",
    "# 1. Find maximum number of pages of results available\n",
    "nytimes_url = generate_nytimes_api_url(\n",
    "    nytimes_query, nytimes_begin_date, nytimes_end_date, nytimes_api\n",
    ").format(0)\n",
    "nytimes_max_pages_returned = math.ceil(\n",
    "    requests.get(nytimes_url).json()[\"response\"][\"meta\"][\"hits\"] / 10\n",
    ")\n",
    "# 2. Set the maximum page number to be queried\n",
    "if nytimes_num_pages_wanted == -1:\n",
    "    nytimes_max_page_num = nytimes_max_pages_returned\n",
    "    nytimes_pages_to_use = \"all available\"\n",
    "else:\n",
    "    nytimes_max_page_num = nytimes_start_page_num + nytimes_num_pages_wanted\n",
    "    nytimes_pages_to_use = \"requested\"\n",
    "print(\n",
    "    f\"Retrieving articles from {nytimes_pages_to_use} pages, \"\n",
    "    f\"number of requested pages ({nytimes_num_pages_wanted}), \"\n",
    "    f\"number of available pages ({nytimes_max_page_num})\"\n",
    ")\n",
    "# Loop over all pages to be queried and retrieve article details\n",
    "for page in range(\n",
    "    nytimes_start_page_num, nytimes_start_page_num + nytimes_max_page_num\n",
    "):\n",
    "    # 3. Send GET request to API and retrieve response\n",
    "    # print(page)\n",
    "    nytimes_url = generate_nytimes_api_url(\n",
    "        nytimes_query, nytimes_begin_date, nytimes_end_date, nytimes_api\n",
    "    )\n",
    "    try:\n",
    "        r = requests.get(nytimes_url.format(page))\n",
    "        # print(r.json().keys())\n",
    "        rdocs = r.json()[\"response\"][\"docs\"]\n",
    "        print(f\"Page: {page}, Found: {len(rdocs)}\")\n",
    "        d = {}\n",
    "        # 4. Extract various attributes of response json and store in dict\n",
    "        for key in [\n",
    "            \"web_url\",\n",
    "            \"lead_paragraph\",\n",
    "            \"abstract\",\n",
    "            \"snippet\",\n",
    "            \"source\",\n",
    "            \"document_type\",\n",
    "            \"news_desk\",\n",
    "            \"section_name\",\n",
    "            \"type_of_material\",\n",
    "            \"subsection_name\",\n",
    "            \"word_count\",\n",
    "        ]:\n",
    "            d[key] = []\n",
    "            for rr in rdocs:\n",
    "                try:\n",
    "                    rr[key]\n",
    "                    d[key].append(rr[key])\n",
    "                except Exception as e:\n",
    "                    d[key].append(None)\n",
    "        print(f\"Retrieved NY Times article details from page number {page}\")\n",
    "        # 5. Convert dict of urls to DataFrame of urls\n",
    "        df_nytimes_article_details = pd.DataFrame.from_dict(d, orient=\"index\").T\n",
    "        # 6. Append page number of DataFrame\n",
    "        df_nytimes_article_details[\"page\"] = page\n",
    "        dfs_nytimes_article_details.append(df_nytimes_article_details)\n",
    "        # Pause between pages\n",
    "        if page != (nytimes_start_page_num + nytimes_num_pages_wanted) - 1:\n",
    "            print(f\"Pausing for 7 seconds before retrieving details from page {page+1}\\n\")\n",
    "            sleep(7)\n",
    "    except Exception as e:\n",
    "        if r.json()[\"errors\"]:\n",
    "            print(f\"Requested page number ({page}) exceeds returned number of pages\")\n",
    "# 7. Concatenate DataFrames across all pages\n",
    "dfs_nytimes_article_details_all = pd.concat(\n",
    "    dfs_nytimes_article_details, axis=0, ignore_index=True\n",
    ").drop_duplicates()\n",
    "# 8. Filter DataFrame to retain articles\n",
    "dfs_nytimes_article_details_all = dfs_nytimes_article_details_all.query('document_type == \"article\"')\n",
    "print(dfs_nytimes_article_details_all.shape[0])\n",
    "display(dfs_nytimes_article_details_all)\n",
    "# 9. Export DataFrame of metadata to *.csv file\n",
    "dfs_nytimes_article_details_all.to_csv(list_of_urls_file[\"nytimes\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
