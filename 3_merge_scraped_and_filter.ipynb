{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Merge and filter data](#merge-and-filter-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from glob import glob\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Merge and filter Hubble data](#merge-and-filter-hubble-data)\n",
    "3. [Merge and filter NYTimes data](#merge-and-filter-nytimes-data)\n",
    "4. [Merge and filter Guardian data](#merge-and-filter-guardian-data)\n",
    "5. [Merge and filter Space.com data](#merge-and-filter-space.com-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will merge scraped listings data (including metadata), from various news publications and stored in `data/raw`, into a separate `data/processed/<publication_name>_processed.csv` file per news publication and filter out any articles that are less than 500 words in length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# General inputs\n",
    "PROJ_ROOT_DIR = os.getcwd()\n",
    "data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\n",
    "processed_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\n",
    "az_storage_container_name = \"myconedesx7\"\n",
    "cloud_data = True\n",
    "\n",
    "# Hubble Filenames\n",
    "# # Local files\n",
    "hubble_filename = \"hubble_urls.csv\"\n",
    "hubble_text = \"hubble.csv\"\n",
    "# # Cloud-based files\n",
    "hubble_inputs = {\n",
    "    \"blobedesz23\": \"urls\",\n",
    "    \"blobedesz22\": \"text\",\n",
    "}\n",
    "hubble_processed_filename = \"hubble_processed.csv\"\n",
    "\n",
    "# NY Times Filenames\n",
    "# # Local files\n",
    "nytimes_filename = \"nytimes_urls__*.csv\"\n",
    "nytimes_text_filenames = [\n",
    "    \"nytimes.csv\",\n",
    "    # # use below if you scrape only certain articles' text at once\n",
    "    # # and then want to combine all tries together\n",
    "    # \"nytimes_1.csv\",\n",
    "    # \"nytimes_3.csv\",\n",
    "    # \"nytimes_2.csv\"\n",
    "]\n",
    "# # Cloud-based files\n",
    "nytimes_inputs = {\n",
    "    \"blobedesz27\": \"urls_1950_1989\",\n",
    "    \"blobedesz28\": \"urls_1990_1999\",\n",
    "    \"blobedesz29\": \"urls_2000_2019\",\n",
    "    \"blobedesz24\": \"text1\",\n",
    "    \"blobedesz25\": \"text2\",\n",
    "    \"blobedesz26\": \"text3\",\n",
    "}\n",
    "nytimes_processed_filename = \"nytimes_processed.csv\"\n",
    "\n",
    "# Space.com Filenames\n",
    "# # Local files\n",
    "space_filename = \"space_com_urls.csv\"\n",
    "space_text_filenames = [\n",
    "    \"space.csv\",\n",
    "    # # use below if you scrape only certain articles' text at once\n",
    "    # # and then want to combine all tries together\n",
    "    # \"space_1.csv\",\n",
    "    # \"space_2.csv\",\n",
    "    # \"space_3.csv\",\n",
    "    # \"space_4.csv\",\n",
    "    # \"space_5.csv\",\n",
    "]\n",
    "# # Cloud-based files\n",
    "space_inputs = {\n",
    "    \"blobedesz35\": \"urls\",\n",
    "    \"blobedesz30\": \"text1\",\n",
    "    \"blobedesz31\": \"text2\",\n",
    "    \"blobedesz32\": \"text3\",\n",
    "    \"blobedesz33\": \"text4\",\n",
    "    \"blobedesz34\": \"text5\",\n",
    "}\n",
    "space_processed_filename = \"space_processed.csv\"\n",
    "\n",
    "# Guardian Filenames\n",
    "# # Local files\n",
    "guardian_filename = \"guardian_urls.csv\"\n",
    "guardian_text_filenames = [\n",
    "    \"guardian.csv\"\n",
    "    # # use below if you scrape only certain articles' text at once\n",
    "    # # and then want to combine all tries together\n",
    "    # \"guardian_1.csv\",\n",
    "    # \"guardian_2.csv\"\n",
    "]\n",
    "# # Cloud-based files\n",
    "guardian_inputs = {\n",
    "    \"blobedesz21\": \"urls\",\n",
    "    \"blobedesz19\": \"text1\",\n",
    "    \"blobedesz20\": \"text2\",\n",
    "}\n",
    "guardian_processed_filename = \"guardian_processed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = (\n",
    "    \"DefaultEndpointsProtocol=https;\"\n",
    "    f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "    f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "    f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    ")\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge-and-filter-hubble-data\"></a>\n",
    "\n",
    "## 2. [Merge and filter Hubble data](#merge-and-filter-hubble-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the scraped text and listings urls from the Hubble website into separate `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cloud_data:\n",
    "    df_hubble_listings = pd.read_csv(os.path.join(data_dir, hubble_filename))\n",
    "    df_hubble_text = pd.read_csv(os.path.join(processed_data_dir, hubble_text))\n",
    "else:\n",
    "    hubble_dict = {}\n",
    "    for az_blob_name, file_type in hubble_inputs.items():\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=az_blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "        hubble_dict[file_type] = pd.read_csv(StringIO(blobstring))\n",
    "    df_hubble_text = pd.concat([v for k,v in hubble_dict.items() if \"text\" in k])\n",
    "    df_hubble_listings = pd.concat([v for k,v in hubble_dict.items() if k == \"urls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble_listings.rename(\n",
    "    columns={\"publication\": \"publication_date\", \"mission\": \"publication\"}, inplace=True\n",
    ")\n",
    "df_hubble_text.drop([\"publication_date\"], axis=1, inplace=True)\n",
    "display(df_hubble_listings.head(2))\n",
    "display(df_hubble_text.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set an index for the `DataFrame`s so that we can join each `DataFrame` on its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble_text = df_hubble_text.set_index([\"url\", \"publication\"])\n",
    "df_hubble_listings = df_hubble_listings.set_index([\"url\", \"publication\"])\n",
    "print(df_hubble_text.shape)\n",
    "display(df_hubble_text.head(2))\n",
    "print(df_hubble_listings.shape)\n",
    "display(df_hubble_listings.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll merge the `DataFrame`s on the index and reset the index so that these index columns appear in the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble = df_hubble_text.merge(\n",
    "    df_hubble_listings, left_index=True, right_index=True, how=\"inner\",\n",
    ").reset_index(drop=False)\n",
    "print(df_hubble.shape)\n",
    "display(df_hubble.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll append `datetime` attributes as columns to the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [\"year\", \"month\", \"day\", \"dayofweek\", \"dayofyear\", \"weekofyear\", \"quarter\"]\n",
    "df_hubble.drop(L, axis=1, inplace=True)\n",
    "df_hubble[\"publication_date\"] = pd.to_datetime(df_hubble[\"publication_date\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble = df_hubble.join(\n",
    "    pd.concat(\n",
    "        (getattr(df_hubble[\"publication_date\"].dt, i).rename(i) for i in L), axis=1\n",
    "    )\n",
    ")\n",
    "df_hubble[\"decade\"] = df_hubble[\"year\"] // 10 * 10\n",
    "print(df_hubble.shape)\n",
    "df_hubble.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll filter out news articles of less than 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble = df_hubble[(df_hubble[\"text\"].str.len() > 500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll drop unwanted columns from the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_hubble_cols = [\n",
    "    \"url\",\n",
    "    \"publication\",\n",
    "    \"news_id\",\n",
    "    \"publication_date\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"weekofyear\",\n",
    "    \"quarter\",\n",
    "]\n",
    "df_hubble.drop(unwanted_hubble_cols, axis=1, inplace=True)\n",
    "print(df_hubble.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll export the merged `DataFrame` to a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_hubble.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")\n",
    "df_hubble.to_csv(\n",
    "    os.path.join(processed_data_dir, hubble_processed_filename), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubble_loaded = pd.read_csv(\n",
    "    os.path.join(processed_data_dir, hubble_processed_filename)\n",
    ")\n",
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_hubble_loaded.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge-and-filter-nytimes-data\"></a>\n",
    "\n",
    "## 3. [Merge and filter NYTimes data](#merge-and-filter-nytimes-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the scraped text and listings urls from the New York Times website into separate `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cloud_data:\n",
    "    df_nytimes_listings = pd.concat(\n",
    "        [pd.read_csv(os.path.join(data_dir, f)) for f in glob(os.path.join(data_dir, nytimes_filename))]\n",
    "    )\n",
    "    df_nytimes_text = pd.concat([pd.read_csv(os.path.join(data_dir, f)) for f in nytimes_text_filenames])\n",
    "else:\n",
    "    nytimes_dict = {}\n",
    "    for az_blob_name, file_type in nytimes_inputs.items():\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=az_blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "        nytimes_dict[file_type] = pd.read_csv(StringIO(blobstring))\n",
    "    df_nytimes_text = pd.concat([v for k,v in nytimes_dict.items() if \"text\" in k])\n",
    "    df_nytimes_listings = pd.concat([v for k,v in nytimes_dict.items() if \"urls\" in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes_listings.rename(\n",
    "    columns={\"web_url\": \"url\", \"source\": \"publication\"}, inplace=True\n",
    ")\n",
    "df_nytimes_text[\"publication\"] = df_nytimes_text[\"publication\"].str.replace(\n",
    "    \"nytimes\", \"The New York Times\"\n",
    ")\n",
    "display(df_nytimes_listings.head(2))\n",
    "display(df_nytimes_text.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set an index for the `DataFrame`s so that we can join each  `DataFrame` on its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes_text = df_nytimes_text.set_index([\"url\", \"publication\"])\n",
    "df_nytimes_listings = df_nytimes_listings.set_index([\"url\", \"publication\"])\n",
    "print(df_nytimes_text.shape)\n",
    "display(df_nytimes_text.head(2))\n",
    "print(df_nytimes_listings.shape)\n",
    "display(df_nytimes_listings.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll merge the `DataFrame`s on the index and reset the index so that these index columns appear in the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes = df_nytimes_text.merge(\n",
    "    df_nytimes_listings, left_index=True, right_index=True, how=\"inner\",\n",
    ").reset_index(drop=False)\n",
    "print(df_nytimes.shape)\n",
    "display(df_nytimes.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll append `datetime` attributes as columns to the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes_text[\"decade\"] = df_nytimes_text[\"year\"] // 10 * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll filter out news articles of less than 500 characters and exclude unwanted articles (i.e. we'll remove articles from the subsection `Environment`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes = df_nytimes[\n",
    "    (df_nytimes[\"type_of_material\"] == \"News\")\n",
    "    & (df_nytimes[\"subsection_name\"] != \"Environment\")\n",
    "    & (df_nytimes[\"text\"].str.len() > 500)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll drop unwanted columns from the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_nytimes_cols = [\n",
    "    \"url\",\n",
    "    \"section_name\",\n",
    "    \"page\",\n",
    "    \"news_desk\",\n",
    "    \"document_type\",\n",
    "    \"type_of_material\",\n",
    "    \"publication_date\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"weekofyear\",\n",
    "    \"quarter\",\n",
    "    \"lead_paragraph\",\n",
    "]\n",
    "df_nytimes.drop(unwanted_nytimes_cols, axis=1, inplace=True)\n",
    "print(df_nytimes.shape)\n",
    "df_nytimes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll export the merged `DataFrame` to a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_nytimes.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")\n",
    "df_nytimes.to_csv(\n",
    "    os.path.join(processed_data_dir, nytimes_processed_filename), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytimes_loaded = pd.read_csv(\n",
    "    os.path.join(processed_data_dir, nytimes_processed_filename)\n",
    ")\n",
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_nytimes_loaded.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge-and-filter-guardian-data\"></a>\n",
    "\n",
    "## 4. [Merge and filter Guardian data](#merge-and-filter-guardian-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the scraped text and listings urls from the Guardian website into separate `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cloud_data:\n",
    "    df_guardian_listings = pd.read_csv(os.path.join(data_dir, guardian_filename))\n",
    "    df_guardian_text = pd.concat(\n",
    "        [pd.read_csv(os.path.join(data_dir, f)) for f in guardian_text_filenames]\n",
    "    )\n",
    "else:\n",
    "    guardian_dict = {}\n",
    "    for az_blob_name, file_type in guardian_inputs.items():\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=az_blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "        guardian_dict[file_type] = pd.read_csv(StringIO(blobstring))\n",
    "    df_guardian_text = pd.concat([v for k,v in guardian_dict.items() if \"text\" in k])\n",
    "    df_guardian_listings = pd.concat([v for k,v in guardian_dict.items() if k == \"urls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian_listings.rename(\n",
    "    columns={\"webUrl\": \"url\", \"webPublicationDate\": \"publication_date\"}, inplace=True\n",
    ")\n",
    "df_guardian_text.drop([\"publication_date\"], axis=1, inplace=True)\n",
    "display(df_guardian_listings.head(2))\n",
    "display(df_guardian_text.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set an index for the `DataFrame`s so that we can join each `DataFrame` on its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian_text = df_guardian_text.set_index([\"url\"])\n",
    "df_guardian_listings = df_guardian_listings.set_index([\"url\"])\n",
    "print(df_guardian_text.shape)\n",
    "display(df_guardian_text.head(2))\n",
    "print(df_guardian_listings.shape)\n",
    "display(df_guardian_listings.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll merge the `DataFrame`s on the index and reset the index so that these index columns appear in the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian = df_guardian_text.merge(\n",
    "    df_guardian_listings, left_index=True, right_index=True, how=\"inner\",\n",
    ").reset_index(drop=False)\n",
    "print(df_guardian.shape)\n",
    "display(df_guardian.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll append `datetime` attributes as columns to the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [\"year\", \"month\", \"day\", \"dayofweek\", \"dayofyear\", \"weekofyear\", \"quarter\"]\n",
    "df_guardian.drop(L, axis=1, inplace=True)\n",
    "df_guardian[\"publication_date\"] = pd.to_datetime(\n",
    "    df_guardian[\"publication_date\"], utc=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian = df_guardian.join(\n",
    "    pd.concat(\n",
    "        (getattr(df_guardian[\"publication_date\"].dt, i).rename(i) for i in L), axis=1\n",
    "    )\n",
    ")\n",
    "df_guardian[\"decade\"] = df_guardian[\"year\"] // 10 * 10\n",
    "print(df_guardian.shape)\n",
    "df_guardian.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll filter out news articles of less than 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian = df_guardian[df_guardian[\"text\"].str.len() > 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll drop unwanted columns from the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_guardian_cols = [\n",
    "    \"url\",\n",
    "    \"id\",\n",
    "    \"sectionId\",\n",
    "    \"sectionName\",\n",
    "    \"type\",\n",
    "    \"isHosted\",\n",
    "    \"pillarId\",\n",
    "    \"pillarName\",\n",
    "    \"page\",\n",
    "    \"publication_date\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"weekofyear\",\n",
    "    \"quarter\",\n",
    "]\n",
    "df_guardian.drop(unwanted_guardian_cols, axis=1, inplace=True)\n",
    "print(df_guardian.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll export the merged `DataFrame` to a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_guardian.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")\n",
    "df_guardian.to_csv(\n",
    "    os.path.join(processed_data_dir, guardian_processed_filename), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_guardian_loaded = pd.read_csv(\n",
    "    os.path.join(processed_data_dir, guardian_processed_filename)\n",
    ")\n",
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_guardian_loaded.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merge-and-filter-space.com-data\"></a>\n",
    "\n",
    "## 5. [Merge and filter Space.com data](#merge-and-filter-space.com-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the scraped text and listings urls from the Space.com website into separate `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cloud_data:\n",
    "    df_space_listings = pd.read_csv(os.path.join(data_dir, space_filename))\n",
    "    df_space_text = pd.concat([pd.read_csv(os.path.join(data_dir, f)) for f in space_text_filenames])\n",
    "else:\n",
    "    space_dict = {}\n",
    "    for az_blob_name, file_type in space_inputs.items():\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=az_storage_container_name, blob=az_blob_name\n",
    "        )\n",
    "        blobstring = blob_client.download_blob().content_as_text()\n",
    "        space_dict[file_type] = pd.read_csv(StringIO(blobstring))\n",
    "    df_space_text = pd.concat([v for k,v in space_dict.items() if \"text\" in k])\n",
    "    df_space_listings = pd.concat([v for k,v in space_dict.items() if k == \"urls\"])\n",
    "# df_space_text.drop([\"publication_date\"], axis=1, inplace=True)\n",
    "display(df_space_listings.head(2))\n",
    "display(df_space_text.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set an index for the `DataFrame`s so that we can join each on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space_text = df_space_text.set_index([\"url\"])\n",
    "df_space_listings = df_space_listings.set_index([\"url\"])\n",
    "print(df_space_text.shape)\n",
    "display(df_space_text.head(2))\n",
    "print(df_space_listings.shape)\n",
    "display(df_space_listings.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll merge the `DataFrame`s on the index and reset the index so that these index columns appear in the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space = df_space_text.merge(\n",
    "    df_space_listings, left_index=True, right_index=True, how=\"inner\",\n",
    ").reset_index(drop=False)\n",
    "print(df_space.shape)\n",
    "display(df_space.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll append `datetime` attributes as columns to the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space[\"decade\"] = df_space[\"year\"] // 10 * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll filter out news articles of less than 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space = df_space[df_space[\"text\"].str.len() > 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll drop unwanted columns from the merged `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_space_cols = [\n",
    "    \"url\",\n",
    "    \"publication_date\",\n",
    "    \"publication\",\n",
    "    \"archive_url\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"dayofyear\",\n",
    "    \"weekofyear\",\n",
    "    \"quarter\",\n",
    "]\n",
    "df_space.drop(unwanted_space_cols, axis=1, inplace=True)\n",
    "print(df_space.shape)\n",
    "df_space.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll export the merged `DataFrame` to a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_space.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")\n",
    "df_space.to_csv(os.path.join(processed_data_dir, space_processed_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space_loaded = pd.read_csv(\n",
    "    os.path.join(processed_data_dir, space_processed_filename)\n",
    ")\n",
    "print(\n",
    "    f\"Memory footprint of DataFrame: {(df_space_loaded.memory_usage().sum() / 1000 / 1000):.2f} MB\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
