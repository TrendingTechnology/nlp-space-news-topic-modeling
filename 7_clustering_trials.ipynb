{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clustering trials](#clustering-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.pipe_helpers\n",
    "from src.pipe_helpers import TextCleaner\n",
    "\n",
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import altair_datetime_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 26\n",
    "MEDIUM_SIZE = 28\n",
    "BIGGER_SIZE = 30\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load joined data](#load-joined-data)\n",
    "3. [Create clustering pipeline](#create-clustering-pipeline)\n",
    "4. [Clustering](#clustering)\n",
    "   - 4.1. [Analysis](#analysis)\n",
    "   - 4.2. [Exploring clustering topics combined with source data](#exploring-clustering-topics-combined-with-source-data)\n",
    "5. [LSA before clustering](#lsa-before-clustering)\n",
    "   - 5.1. [Analysis](#analysis)\n",
    "   - 5.2. [Exploring LSA-clustering topics combined with source data](#exploring-lsa-clustering-topics-combined-with-source-data)\n",
    "6. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will conduct experiments with the [`KMeans` clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering) to extract clusters from on the joined news listings data in `data/processed/*_processed.csv`.\n",
    "\n",
    "The articles can be clustered by their topic, using a [bag of words model](https://en.wikipedia.org/wiki/Bag-of-words_model). Feature extraction will be done using [TFIDF vectorization](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). We can compare the determined clusters to the extracted topics found using NLP approaches in earlier notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define below the variables and helper functions that are to be used throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.path.abspath(os.getcwd())\n",
    "processed_data_dir = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "publication_name = \"guardian\"\n",
    "\n",
    "# Data locations\n",
    "data_dir_path = os.path.join(processed_data_dir, f\"{publication_name}_processed.csv\")\n",
    "cloud_run = True\n",
    "\n",
    "# Custom stop words to include\n",
    "manual_stop_words = [\"nt\", \"ll\", \"ve\"]\n",
    "\n",
    "# Cluster naming\n",
    "mapping_dict = {\n",
    "    \"guardian\": {\n",
    "        0: \"Gravity and Black Holes - Hawking\",\n",
    "        1: \"Shuttle Missions and Crashes\",\n",
    "        2: \"Global Warming\",\n",
    "        3: \"Academia 2\",\n",
    "        4: \"Studying Comets and Meteors\",\n",
    "        5: \"Rocket Launches - Testing\",\n",
    "        6: \"Discover of Sub-Atomic particles\",\n",
    "        7: \"Academia 1\",\n",
    "        8: \"Planetary Research\",\n",
    "        9: \"Imaging Stars - Astronomy\",\n",
    "        10: \"Objects crashing into Earth\",\n",
    "        11: \"Rocket Launches - Moon Landing\",\n",
    "        12: \"Sky Watching\",\n",
    "        13: \"ISS - USA and Russian segment\",\n",
    "        14: \"Mars Exploration\",\n",
    "    }\n",
    "}\n",
    "mapping_dict_lsa = {\n",
    "    \"guardian\": {\n",
    "        0: \"Academia 1\",\n",
    "        1: \"Space Funding Bodies\",\n",
    "        2: \"Studying Comets and Meteors\",\n",
    "        3: \"Academia 2\",\n",
    "        4: \"ISS - USA and Russian segment\",\n",
    "        5: \"Shuttle Missions and Crashes\",\n",
    "        6: \"Mars Exploration\",\n",
    "        7: \"Planetary Research\",\n",
    "        8: \"Imaging Stars - Astronomy\",\n",
    "        9: \"Discover of Sub-Atomic particles\",\n",
    "        10: \"Sky Watching\",\n",
    "        11: \"Rocket Launches - Moon Landing\",\n",
    "        12: \"Gravity and Black Holes - Hawking\",\n",
    "        13: \"Rocket Launches - Testing\",\n",
    "        14: \"Objects crashing into Earth\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# General inputs\n",
    "minibatch = False\n",
    "kmeans_random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict[publication_name] = {int(k): v for k, v in mapping_dict[publication_name].items() if type(k) != int}\n",
    "mapping_dict_lsa[publication_name] = {int(k): v for k, v in mapping_dict_lsa[publication_name].items() if type(k) != int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words from all packages\n",
    "# NLTK\n",
    "nltk_dir = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "if not os.path.isdir(nltk_dir):\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "# Spacy and sklearn\n",
    "spacy_stop_words = STOP_WORDS\n",
    "sklearn_stop_words = stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Assemble manual list of stop words\n",
    "spacy_not_in_sklearn = set(spacy_stop_words) - set(sklearn_stop_words)\n",
    "nltk_not_in_sklearn = set(nltk_stop_words) - set(sklearn_stop_words)\n",
    "all_stop_words = set(\n",
    "    list(set(sklearn_stop_words))\n",
    "    + list(spacy_not_in_sklearn)\n",
    "    + list(nltk_not_in_sklearn)\n",
    ")\n",
    "\n",
    "# Manually add to stop words\n",
    "for manual_stop_word in manual_stop_words:\n",
    "    all_stop_words.add(manual_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-joined-data\"></a>\n",
    "\n",
    "## 2. [Load joined data](#load-joined-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous notebooks, we'll start by loading the joined data from from a publication, stored at `data/processed/<publication-name>_processed.csv`, into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir_path)\n",
    "df = df[[\"text\", \"year\"]]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = df.loc[:, \"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-clustering-pipeline\"></a>\n",
    "\n",
    "## 3. [Create clustering pipeline](#create-clustering-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the a clustering pipeline using the [`KMeans` clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if minibatch:\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=len(mapping_dict[publication_name]),\n",
    "        init=\"k-means++\",\n",
    "        n_init=10,\n",
    "        init_size=3000,\n",
    "        batch_size=3000,\n",
    "        verbose=False,\n",
    "        random_state=kmeans_random_state,\n",
    "    )\n",
    "else:\n",
    "    km = KMeans(\n",
    "        n_clusters=len(mapping_dict[publication_name]),\n",
    "        init=\"k-means++\",\n",
    "        max_iter=500,\n",
    "        n_init=10,\n",
    "        verbose=False,\n",
    "        random_state=kmeans_random_state,\n",
    "    )\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    preprocessor=preprocessing,\n",
    "    stop_words=all_stop_words,  # \"all_stop_words\" or \"english\"\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    binary=False,\n",
    "    strip_accents=\"ascii\",\n",
    "    token_pattern=\"[a-z][a-z]+\",\n",
    ")\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"cleaner\", TextCleaner(split=False)),\n",
    "        (\"vectorizer\", vectorizer),\n",
    "        (\"kmeans\", km),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "\n",
    "## 4. [Clustering](#clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "\n",
    "### 4.1. [Analysis](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the clustering analysis pipeline to cluster the documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "_ = pipe.fit_transform(corpus_raw)\n",
    "X_vectors = pipe.named_steps[\"vectorizer\"].transform(corpus_raw)\n",
    "silhouette_score_calc = silhouette_score(\n",
    "    X_vectors, pipe.named_steps[\"kmeans\"].labels_, sample_size=df.shape[0]\n",
    ")\n",
    "print(f\"Silhouette Coefficient: {silhouette_score_calc:.3f}\")\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, are the top 10 terms per cluster created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top terms per cluster, using random_state={kmeans_random_state}:\")\n",
    "order_centroids = pipe.named_steps[\"kmeans\"].cluster_centers_.argsort()[:, ::-1]\n",
    "terms = pipe.named_steps[\"vectorizer\"].get_feature_names()\n",
    "for i in range(len(mapping_dict[publication_name])):\n",
    "    print(f\"Cluster {i:0d}:\", end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" {terms[ind]}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are shown below, for the pre-determined choice of `random_state` of `42` specified in the `KMeans` clustering algorithm above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Top terms per cluster, using random_state=42:\n",
    "Cluster 0: stars telescope black galaxies universe galaxy star light hubble gravitational\n",
    "Cluster 1: shuttle space nasa launch station astronauts columbia mission crew said\n",
    "Cluster 2: climate life ice change earth water carbon warming global scientists\n",
    "Cluster 3: brain cells brains memory people memories body human patients neurons\n",
    "Cluster 4: comet rosetta philae comets lander dust mission sun surface solar\n",
    "Cluster 5: space rocket launch satellites satellite spacex flight said company orbit\n",
    "Cluster 6: universe particles particle higgs matter theory physics dark lhc energy\n",
    "Cluster 7: science space people research said new like says time world\n",
    "Cluster 8: solar sun pluto planet earth spacecraft venus cassini planets said\n",
    "Cluster 9: planets planet star stars kepler life astronomers habitable telescope earth\n",
    "Cluster 10: asteroid asteroids earth impact space hit spacecraft rock eros object\n",
    "Cluster 11: moon lunar space apollo armstrong moons mission nasa astronauts surface\n",
    "Cluster 12: eclipse iss shadow evening predictions asterisks moon directions earths sky\n",
    "Cluster 13: station space mir russian crew peake astronaut said astronauts iss\n",
    "Cluster 14: mars martian planet life mission rover water surface lander said\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Generally, these divisions/clusters are similar to those found in the NMF (`sklearn`) or CorEx approaches.\n",
    "2. One notable exception is that none of the clusters correspond to the topic `Space Funding Bodies`. Analysis of the number of occurrences of this as the most popular topic, over the decades, is not possible.\n",
    "   - in its place is `Sky Watching`, which is very similar to `Imaging Stars - Astronomy`\n",
    "3. The `Saturn Research` topic appears to have been merged into `Academia` - the research element is a common theme in both and is likely driving this combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [distance of each document from its associated cluster center](https://stackoverflow.com/questions/54240144/distance-between-nodes-and-the-centroid-in-a-kmeans-cluster/54247525#54247525) is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "X_dist = pipe.named_steps[\"kmeans\"].fit_transform(X_vectors) ** 2\n",
    "df[\"sqdist\"] = pd.Series(X_dist.sum(axis=1).round(2))\n",
    "display(df.head(10))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now [add](https://stackoverflow.com/a/50804098/4057186) the topic name (names assigned to each cluster, based on its component terms) found, to each document the above as a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of points for each cluster\n",
    "cluster_topic_name_mapper_dict = {\n",
    "    k: np.where(pipe.named_steps[\"kmeans\"].labels_ == k)[0].tolist()\n",
    "    for k in range(pipe.named_steps[\"kmeans\"].n_clusters)\n",
    "}\n",
    "# Replace cluster indices by manually determined names\n",
    "df[\"most_popular_topic\"] = np.nan\n",
    "for k, v in cluster_topic_name_mapper_dict.items():\n",
    "    df.loc[v, \"most_popular_topic\"] = mapping_dict[publication_name][k]\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-clustering-topics-combined-with-source-data\"></a>\n",
    "\n",
    "### 4.2. [Exploring clustering topics combined with source data](#exploring-clustering-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will show a heatmap of the most popular topic by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df.groupby([\"most_popular_topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"most_popular_topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "display(topics_by_timeframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"most_popular_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Most popular topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\n",
    "            \"title\": \"Most popular topic\",\n",
    "            \"field\": \"most_popular_topic\",\n",
    "            \"type\": \"nominal\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as main topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=os.path.join(PROJ_ROOT_DIR, \"reports\", \"figures\", \"my_heatmap.html\"),\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lsa-before-clustering\"></a>\n",
    "\n",
    "## 5. [LSA before clustering](#lsa-before-clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) is Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preliminary attempt at implementing this, before `KMeans` clustering, to obtain topics after TFIDF vectorization of the documents. This will be done using [Truncated SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) in the `sklearn` Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analysis\"></a>\n",
    "\n",
    "### 5.1. [Analysis](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a new pipeline to support LSA before `KMeans` clustering\n",
    "- the only change here is to add a SVD step before the clustering step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_trunc_svd = Pipeline(\n",
    "    steps=[\n",
    "        (\"cleaner\", TextCleaner(split=False)),\n",
    "        (\"vectorizer\", vectorizer),\n",
    "        (\n",
    "            \"svd\",\n",
    "            TruncatedSVD(\n",
    "                len(mapping_dict[publication_name]), random_state=kmeans_random_state\n",
    "            ),\n",
    "        ),\n",
    "        (\"kmeans\", km),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the LSA+clustering analysis pipeline to cluster the documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "_ = pipe_trunc_svd.fit_transform(corpus_raw)\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, are the top 10 terms per cluster created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_space_centroids = pipe_trunc_svd.named_steps[\"svd\"].inverse_transform(\n",
    "    pipe_trunc_svd.named_steps[\"kmeans\"].cluster_centers_\n",
    ")\n",
    "print(f\"Top terms per cluster, using random_state={kmeans_random_state}:\")\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = pipe_trunc_svd.named_steps[\"vectorizer\"].get_feature_names()\n",
    "for i in range(len(mapping_dict[publication_name])):\n",
    "    print(f\"Cluster {i:0d}:\", end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" {terms[ind]}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters are shown below, for the pre-determined choice of `random_state` of `42` specified in both the truncated SVD and `KMeans` clustering algorithm above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Top terms per cluster, using random_state=42:\n",
    "Cluster 0: science people research like says new world brain scientists scientific\n",
    "Cluster 1: space said science satellites uk satellite launch nasa station mission\n",
    "Cluster 2: comet rosetta philae comets lander dust mission surface sun solar\n",
    "Cluster 3: space science people says said like life time earth brain\n",
    "Cluster 4: station space mir russian crew iss astronauts said astronaut international\n",
    "Cluster 5: shuttle nasa space launch station astronauts mission columbia crew said\n",
    "Cluster 6: mars martian life planet mission water surface rover lander nasa\n",
    "Cluster 7: pluto planet solar cassini moon spacecraft planets saturn titan moons\n",
    "Cluster 8: planets planet stars star telescope life astronomers solar earth kepler\n",
    "Cluster 9: higgs particle particles matter lhc universe physics dark energy boson\n",
    "Cluster 10: sun solar earth space satellites eclipse satellite planet orbit atmosphere\n",
    "Cluster 11: moon lunar space apollo nasa astronauts mission moons earth surface\n",
    "Cluster 12: universe stars black galaxies telescope gravitational matter dark light waves\n",
    "Cluster 13: space rocket launch spacex flight company musk rockets said virgin\n",
    "Cluster 14: asteroid asteroids earth impact space rock object collision comet said\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The `Space Funding Bodies` topic reappears, but is not as strongly defined (in terms of its component terms) as in NMF/CorEx approaches.\n",
    "2. Two clusters may be combined\n",
    "   - `Academia 1,2`\n",
    "     - the Saturn Research topic might been replaced by one of these\n",
    "   - `Sky watching` and `Imaging Stars - Astronomy`\n",
    "     - possibly one of these has replaced `Global Warming` from the NMF/CorEx approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percent variance explained by the truncated SVD is calculated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pipe_trunc_svd.named_steps[\"svd\"].explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance of the SVD step: {int(explained_variance * 100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Since the SVD only accounts for 6% of the variance, hyperparameter optimization of all steps in the pipeline should be pursued.\n",
    "   - it is reassuring that the majority of topics reappear despite this (possibly excessively) strong degree of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance of each document from its associated cluster center is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "\n",
    "X_dist = pipe.named_steps[\"kmeans\"].fit_transform(X_vectors) ** 2\n",
    "df[\"sqdist\"] = pd.Series(X_dist.sum(axis=1).round(2))\n",
    "\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(\n",
    "    f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-lsa-clustering-topics-combined-with-source-data\"></a>\n",
    "\n",
    "### 5.2. [Exploring LSA-clustering topics combined with source data](#exploring-lsa-clustering-topics-combined-with-source-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we add a topic name (names assigned to each cluster, based on its component terms), to each document above as a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of points for each cluster\n",
    "cluster_topic_name_mapper_dict = {\n",
    "    k: np.where(pipe.named_steps[\"kmeans\"].labels_ == k)[0].tolist()\n",
    "    for k in range(pipe.named_steps[\"kmeans\"].n_clusters)\n",
    "}\n",
    "# Replace cluster indices by manually determined names\n",
    "df[\"most_popular_topic\"] = np.nan\n",
    "for k, v in cluster_topic_name_mapper_dict.items():\n",
    "    df.loc[v, \"most_popular_topic\"] = mapping_dict_lsa[publication_name][k]\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will show a heatmap of the most popular topic by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_timeframe = (\n",
    "    df.groupby([\"most_popular_topic\", \"year\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .sort_values(by=[\"most_popular_topic\", 0, \"year\"], ascending=False)\n",
    "    .rename(columns={0: \"count\"})\n",
    ")\n",
    "display(topics_by_timeframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altair_datetime_heatmap(\n",
    "    topics_by_timeframe,\n",
    "    x=\"year:O\",\n",
    "    y=\"most_popular_topic:N\",\n",
    "    xtitle=\"Year\",\n",
    "    ytitle=\"Most popular topic\",\n",
    "    tooltip=[\n",
    "        {\"title\": \"Year\", \"field\": \"year\", \"type\": \"ordinal\",},\n",
    "        {\n",
    "            \"title\": \"Most popular topic\",\n",
    "            \"field\": \"most_popular_topic\",\n",
    "            \"type\": \"nominal\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Number of occurrences as main topic\",\n",
    "            \"field\": \"count\",\n",
    "            \"type\": \"quantitative\",\n",
    "        },\n",
    "    ],\n",
    "    cmap=\"yelloworangered\",\n",
    "    legend_title=\"\",\n",
    "    color_by_col=\"count:Q\",\n",
    "    yscale=\"log\",\n",
    "    axis_tick_font_size=12,\n",
    "    axis_title_font_size=16,\n",
    "    title_font_size=20,\n",
    "    legend_fig_padding=10,  # default is 18\n",
    "    y_axis_title_alignment=\"left\",\n",
    "    fwidth=700,\n",
    "    fheight=450,\n",
    "    file_path=os.path.join(PROJ_ROOT_DIR, \"reports\", \"figures\", \"my_heatmap.html\"),\n",
    "    save_to_html=False,\n",
    "    sort_y=[],\n",
    "    sort_x=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will show a bar chart of the number of occurrences of the `\"Space Funding Bodies\"` as the most popular topic, relative to the year 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = (\n",
    "    topics_by_timeframe[\n",
    "        topics_by_timeframe[\"most_popular_topic\"] == \"Space Funding Bodies\"\n",
    "    ]\n",
    "    .set_index(\"year\")[\"count\"]\n",
    "    .sort_index()\n",
    ")\n",
    "funds / funds.loc[funds.index.min()]\n",
    "funds = funds / funds.loc[funds.index.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "funds.plot(kind=\"bar\", ax=ax, rot=45, align=\"edge\", width=0.8)\n",
    "ax.set_title(\n",
    "    \"Cyclic variation in funding as main topic in article\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "h = plt.ylabel(\"Funding\\n(rel. to 1981)\", labelpad=65, fontweight=\"bold\")\n",
    "h.set_rotation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The chart generally agrees with trends seen in other approaches - peaks in articles published occur in 2005 and a broader peak in news publications under this topic centered around the year 2014. The previously seen peak in 2007 does not appear here.\n",
    "2. Although this topic was deemed present as one of the clusters, it is clear (from the weaker peak intensities and smaller number of years with documents) that this cluster is distinct from the topic found in the NMF/CorEx aproaches from the smaller number of years containing published articles assigned to this topic\n",
    "   - as mentioned earlier, tuning hyperparametes of the TFIDF vectorizer, clustering model and truncated SVD is warranted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "\n",
    "## 6. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations and Future Work**\n",
    "1. Two out of fifteen topics found using the NLP (NMF) and CorEx approaches are likely being merged here in terms of clusters. The terms in the other topics appear to lie in clusters here that are distinctly separated from eachother. The optimal number of clusters should be investigated in order to explore `KMeans`' performance as a smaller or larger number of clusters are allowed. This could be compared to the pre-determined choice of 15 clusters used here in and also to the NLP approaches in earlier notebooks.\n",
    "2. When compared to the NMF/CorEx approaches, since the only changes were replacement of an NLP step by clustering or LSA+clustering step, this would suggest that hyperparameters of the newly added steps need tuning. This should be pursued in combination with optimizing hyperparameters of each other step in the pipeline - TFIDF vectorization and (optionally) LSA - as their values for clustering analysis may not necessarily be the same as those used in NLP approaches.\n",
    "4. LSA is helpful to facilitate dimensionality reduction of the TFIDF feature matrix, but could require tuning in order to extract clearly separable clusters that match topics found using NLP techniques (NMF/CorEx). This should be explored in combination with the `max_features` input argument of the TFIDF vectorizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
